{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "## Subsurface Data Analytics \n",
    "\n",
    "### Feature Selection for Subsurface Data Analytics in Python \n",
    "\n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsurface Machine Learning: Feature Ranking for Subsurface Data Analytics \n",
    "\n",
    "Here's a demonstration of feature ranking for subsurface modeling in Python. This is part of my Subsuface Machine Learning Course at the Cockrell School of Engineering at the University of Texas at Austin.  \n",
    "\n",
    "* check out the associated recorded lecture for more details on these methods, [Feature Selection Lecture](https://www.youtube.com/watch?v=5Q0gemu-h3Q).\n",
    "\n",
    "#### Feature Selection / Variable Ranking\n",
    "\n",
    "There are often many predictor features, input variables, available for us to work with for subsurface prediction. There are good reasons to be selective, throwing in every possible feature is not a good idea! In general, for the best prediction model, careful selection of the fewest features that provide the most amount of information is the best practice. \n",
    "\n",
    "Here's why:\n",
    "\n",
    "* more variables result in more complicated workflows that require more professional time and have increased opportunity for blunders\n",
    "* higher dimensional feature sets are more difficult to visualize\n",
    "* more complicated models may be more difficult to interrogate, interpret and QC\n",
    "* inclusion of highly redundant and colinear variables increases model instability and decreases prediction accuracy in testing\n",
    "* more variables generally increase the computational time required to train the model and the model may be less compact and portable\n",
    "* the risk of overfit increases with the more variables, more complexity\n",
    "\n",
    "#### What is Feature Ranking?\n",
    "\n",
    "Feature ranking is a set of metrics that assign relative importance or value to each feature with respect to information contained for inference and importance in predicting a response feature. There are a wide variety of possible methods to accomplish this. My recommendation is a **'wide-array'** approach with multiple analyses and metrics, while understanding the assumptions and limitations of each method.  \n",
    "\n",
    "Here's the general types of metrics that we will consider for feature ranking.\n",
    "\n",
    "1. Visual Inspection of Data Distributions and Scatter Plots\n",
    "2. Statistical Summaries\n",
    "3. Model-based\n",
    "4. Recursive Feature Elimination \n",
    "\n",
    "Also, we should not neglect expert knowledge.  If additional information is known about physical processes, causation, reliability and availability of features this should be integrated into assigning feature ranks.\n",
    "\n",
    "#### Objective \n",
    "\n",
    "In the Stochastic Machine Learning class, I want to provide hands-on experience with solving complicated subsurface modeling problems with data analytics, machine learning. Python provides an excellent vehicle to accomplish this. I have coded a package called GeostatsPy with GSLIB: Geostatistical Library (Deutsch and Journel, 1998) functionality that provides basic building blocks for building subsurface modeling workflows. \n",
    "\n",
    "The objective is to remove the hurdles of subsurface modeling workflow construction by providing building blocks and sufficient examples. This is not a coding class per se, but we need the ability to 'script' workflows working with numerical methods.    \n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "Here's the steps to get setup in Python with the GeostatsPy package:\n",
    "\n",
    "1. Install Anaconda 3 on your machine (https://www.anaconda.com/download/). \n",
    "2. From Anaconda Navigator (within Anaconda3 group), go to the environment tab, click on base (root) green arrow and open a terminal. \n",
    "3. In the terminal type: pip install geostatspy. \n",
    "4. Open Jupyter and in the top block get started by copy and pasting the code block below from this Jupyter Notebook to start using the geostatspy functionality. \n",
    "\n",
    "You will need to copy the data file to your working directory.  They are available here:\n",
    "\n",
    "* Tabular data - unconv_MV_v4.csv at https://git.io/fhHLT.\n",
    "\n",
    "There are examples below with GeostatsPy functions. You can go here to see a list of the available functions, https://git.io/fh4eX, and for other example workflows and source code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geostatspy.GSLIB as GSLIB                        # GSLIB utilies, visualization and wrapper\n",
    "import geostatspy.geostats as geostats                  # GSLIB methods convert to Python        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.21.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy\n",
    "numpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "#!{sys.executable} -m pip install --user numpy==1.21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also need some standard packages. These should have been installed with Anaconda 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                      # ndarrys for gridded data\n",
    "import pandas as pd                                     # DataFrames for tabular data\n",
    "import os                                               # set working directory, run executables\n",
    "import matplotlib.pyplot as plt                         # for plotting\n",
    "from matplotlib.colors import ListedColormap            # custom color maps\n",
    "from matplotlib.ticker import (MultipleLocator, AutoMinorLocator) # control of axes ticks\n",
    "from scipy import stats                                 # summary statistics\n",
    "import math                                             # trigonometry etc.\n",
    "import scipy.signal as signal                           # kernel for moving window calculation\n",
    "import random                                           # for randon numbers\n",
    "import seaborn as sns                                   # for matrix scatter plots\n",
    "from scipy import linalg                                # for linear regression\n",
    "from sklearn import preprocessing                       # remove encoding error\n",
    "from sklearn.feature_selection import RFE               # for recursive feature selection\n",
    "from sklearn.feature_selection import mutual_info_regression # mutual information\n",
    "from sklearn.linear_model import LinearRegression       # linear regression model\n",
    "from sklearn.ensemble import RandomForestRegressor      # model-based feature importance\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor # variance inflation factor\n",
    "plt.rc('axes', axisbelow=True)                          # girds and axes behind all plot elements\n",
    "cmap = plt.cm.inferno                                   # default colormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Design Custom Color Map\n",
    "\n",
    "Accounting for significance by masking nonsignificant values\n",
    "\n",
    "* for demonstration only currently, could be updated for each plot based on results confidence and uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_colormap = plt.cm.get_cmap('RdBu_r', 256)            # make a custom colormap\n",
    "newcolors = my_colormap(np.linspace(0, 1, 256))         # define colormap space\n",
    "white = np.array([250/256, 250/256, 250/256, 1])        # define white color (4 channel)\n",
    "#newcolors[26:230, :] = white                           # mask all correlations less than abs(0.8)\n",
    "#newcolors[56:200, :] = white                           # mask all correlations less than abs(0.6)\n",
    "newcolors[76:180, :] = white                            # mask all correlations less than abs(0.4)\n",
    "signif = ListedColormap(newcolors)                      # assign as listed colormap\n",
    "  \n",
    "my_colormap = plt.cm.get_cmap('inferno', 256)           # make a custom colormap\n",
    "newcolors = my_colormap(np.linspace(0, 1, 256))         # define colormap space\n",
    "white = np.array([250/256, 250/256, 250/256, 1])        # define white color (4 channel)\n",
    "#newcolors[26:230, :] = white                           # mask all correlations less than abs(0.8)\n",
    "newcolors[0:12, :] = white                              # mask all correlations less than abs(0.6)\n",
    "#newcolors[86:170, :] = white                           # mask all correlations less than abs(0.4)\n",
    "sign1 = ListedColormap(newcolors)                       # assign as listed colormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a couple of functions to assist with calculating metrics for ranking and other plots:\n",
    "\n",
    "* **partial_corr** - partial correlation coefficient\n",
    "\n",
    "* **semipar_corr** - semipartial correlation coefficient\n",
    "\n",
    "* **mutual_matrix** - mutual information matrix, matrix of all pairwise mutual information\n",
    "\n",
    "* **mutual_information_objective** - my modified version of the MRMR loss function (Ixy - average(Ixx)) for feature ranking (uses all other predictor features)\n",
    "\n",
    "* **delta_mutual_information_quotient** - change in mutual information quotient by adding and rmeove a specific feature (uses all other predictor features for the comparison) \n",
    "\n",
    "* **weighted_avg_and_std** - average and standard deviation account for data weights\n",
    "\n",
    "* **weighted_percentile** - percentile accounting for data weights\n",
    "\n",
    "* **histogram_bounds** - add confidence intervals to histograms\n",
    "\n",
    "Here are the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partial_corr(C):                                    # partial correlation by Fabian Pedregosa-Izquierdo, f@bianp.net\n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    P_corr = np.zeros((p, p), dtype=float)\n",
    "    for i in range(p):\n",
    "        P_corr[i, i] = 1\n",
    "        for j in range(i+1, p):\n",
    "            idx = np.ones(p, dtype=bool)\n",
    "            idx[i] = False\n",
    "            idx[j] = False\n",
    "            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n",
    "            beta_j = linalg.lstsq(C[:, idx], C[:, i])[0]\n",
    "            res_j = C[:, j] - C[:, idx].dot( beta_i)\n",
    "            res_i = C[:, i] - C[:, idx].dot(beta_j)\n",
    "            corr = stats.pearsonr(res_i, res_j)[0]\n",
    "            P_corr[i, j] = corr\n",
    "            P_corr[j, i] = corr\n",
    "    return P_corr\n",
    "\n",
    "def semipartial_corr(C):                                # Michael Pyrcz modified the function above by Fabian Pedregosa-Izquierdo, f@bianp.net for semipartial correlation\n",
    "    C = np.asarray(C)\n",
    "    p = C.shape[1]\n",
    "    P_corr = np.zeros((p, p), dtype=float)\n",
    "    for i in range(p):\n",
    "        P_corr[i, i] = 1\n",
    "        for j in range(i+1, p):\n",
    "            idx = np.ones(p, dtype=bool)\n",
    "            idx[i] = False\n",
    "            idx[j] = False\n",
    "            beta_i = linalg.lstsq(C[:, idx], C[:, j])[0]\n",
    "            res_j = C[:, j] - C[:, idx].dot( beta_i)\n",
    "            res_i = C[:, i] \n",
    "            corr = stats.pearsonr(res_i, res_j)[0]\n",
    "            P_corr[i, j] = corr\n",
    "            P_corr[j, i] = corr\n",
    "    return P_corr\n",
    "\n",
    "def mutual_matrix(df,features):                         # calculate mutual information matrix\n",
    "    mutual = np.zeros([len(features),len(features)])\n",
    "    for i, ifeature in enumerate(features):\n",
    "        for j, jfeature in enumerate(features):\n",
    "            if i != j:\n",
    "                mutual[i,j] = mutual_info_regression(df.iloc[:,i].values.reshape(-1, 1),np.ravel(df.iloc[:,j].values))[0]\n",
    "    mutual /= np.max(mutual) \n",
    "    for i, ifeature in enumerate(features):\n",
    "        mutual[i,i] = 1.0\n",
    "    return mutual\n",
    "\n",
    "def mutual_information_objective(x,y):                  # modified from MRMR loss function, Ixy - average(Ixx)\n",
    "    mutual_information_quotient = []\n",
    "    for i, icol in enumerate(x.columns):\n",
    "        Vx = mutual_info_regression(x.iloc[:,i].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1)))\n",
    "        Ixx_mat = []\n",
    "        for m, mcol in enumerate(x.columns):\n",
    "            if i != m:\n",
    "                Ixx_mat.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,i].values.reshape(-1, 1))))\n",
    "        Wx = np.average(Ixx_mat)\n",
    "        mutual_information_quotient.append(Vx/Wx)\n",
    "    mutual_information_quotient  = np.asarray(mutual_information_quotient).reshape(-1)\n",
    "    return mutual_information_quotient\n",
    "\n",
    "def delta_mutual_information_quotient(x,y):             # standard mutual information quotient\n",
    "    delta_mutual_information_quotient = []               \n",
    "    \n",
    "    Ixy = []\n",
    "    for m, mcol in enumerate(x.columns):\n",
    "        Ixy.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))\n",
    "    Vs = np.average(Ixy)\n",
    "    Ixx = []\n",
    "    for m, mcol in enumerate(x.columns):\n",
    "        for n, ncol in enumerate(x.columns):\n",
    "            Ixx.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))\n",
    "    Ws = np.average(Ixx) \n",
    "    \n",
    "    for i, icol in enumerate(x.columns):          \n",
    "        Ixy_s = []                                          \n",
    "        for m, mcol in enumerate(x.columns):\n",
    "            if m != i:\n",
    "                Ixy_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(y.values.reshape(-1, 1))))\n",
    "        Vs_s = np.average(Ixy_s)\n",
    "        Ixx_s = []\n",
    "        for m, mcol in enumerate(x.columns):\n",
    "            if m != i:\n",
    "                for n, ncol in enumerate(x.columns):\n",
    "                    if n != i:\n",
    "                        Ixx_s.append(mutual_info_regression(x.iloc[:,m].values.reshape(-1, 1),np.ravel(x.iloc[:,n].values.reshape(-1, 1))))                  \n",
    "        Ws_s = np.average(Ixx_s)\n",
    "        delta_mutual_information_quotient.append((Vs/Ws)-(Vs_s/Ws_s))\n",
    "    \n",
    "    delta_mutual_information_quotient  = np.asarray(delta_mutual_information_quotient).reshape(-1)  \n",
    "    return delta_mutual_information_quotient\n",
    "\n",
    "def weighted_avg_and_std(values, weights):              # calculate weighted statistics (Eric O Lebigot, stack overflow)\n",
    "    average = np.average(values, weights=weights)\n",
    "    variance = np.average((values-average)**2, weights=weights)\n",
    "    return (average, math.sqrt(variance))\n",
    "\n",
    "def weighted_percentile(data, weights, perc):           # calculate weighted percentile (iambr on StackOverflow @ https://stackoverflow.com/questions/21844024/weighted-percentile-using-numpy/32216049) \n",
    "    ix = np.argsort(data)\n",
    "    data = data[ix] \n",
    "    weights = weights[ix] \n",
    "    cdf = (np.cumsum(weights) - 0.5 * weights) / np.sum(weights) \n",
    "    return np.interp(perc, cdf, data)\n",
    "\n",
    "def histogram_bounds(values,weights,color):             # add uncertainty bounds to a histogram          \n",
    "    p10 = weighted_percentile(values,weights,0.1); avg = np.average(values,weights=weights); p90 = weighted_percentile(values,weights,0.9)\n",
    "    plt.plot([p10,p10],[0.0,45],color = color,linestyle='dashed')\n",
    "    plt.plot([avg,avg],[0.0,45],color = color)\n",
    "    plt.plot([p90,p90],[0.0,45],color = color,linestyle='dashed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the working directory\n",
    "\n",
    "I always like to do this so I don't lose files and to simplify subsequent read and writes (avoid including the full address each time). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"d:/PGE383\")                                   # set the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Tabular Data\n",
    "\n",
    "Here's the command to load our comma delimited data file in to a Pandas' DataFrame object.  \n",
    "\n",
    "**Dataset**:\n",
    "\n",
    "0 - subsurface energy dataset, multiple wells in space with multiple features\n",
    "1 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 0\n",
    "\n",
    "if dataset == 0:\n",
    "    #df = pd.read_csv('unconv_MV_v4.csv')               # load our data table\n",
    "    df = pd.read_csv('https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV_v4.csv') # load data from Dr. Pyrcz's GitHub respository\n",
    "    #df = df.rename(columns={'Production':'Prod'})\n",
    "    response = 'Prod'\n",
    "\n",
    "    x = df.copy(deep = True); x = x.drop(response,axis='columns')\n",
    "    Y = df.loc[:,response]\n",
    "    \n",
    "    pred = x.columns\n",
    "    resp = Y.name\n",
    "    \n",
    "    xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax = [24.0,10.0,5.0,85.0,2.2,2.9]\n",
    "    Ymin = 500.0; Ymax = 9000.0\n",
    "    \n",
    "    predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)',\n",
    "                 'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n",
    "    resplabel = 'Normalized Initial Production (MCFPD)'\n",
    "    \n",
    "    predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio',\n",
    "                 'Total Organic Carbon','Vitrinite Reflectance']\n",
    "    resptitle = 'Normalized Initial Production'\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the DataFrame would be useful and we already learned about these methods in this demo (https://git.io/fNgRW). \n",
    "\n",
    "We can preview the DataFrame by utilizing the 'head' DataFrame member function (with a nice and clean format, see below). With the head command, add parameter 'n=13' to see the first 13 rows of the dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(n=13)                                           # we could also use this command for a table preview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has features from 200 unconventional wells including: \n",
    "\n",
    "0. well index\n",
    "1. well average porosity (%) \n",
    "2. permeability (mD)\n",
    "3. accoustic impedance (kg/m2s*10^6)\n",
    "4. brittness ratio (%) \n",
    "5. total organic carbon (%) \n",
    "6. vitrinite reflectance (%)\n",
    "8. normalized initial production 90 day average (MCFPD). \n",
    "\n",
    "Note, the dataset is synthetic, but has realistic ranges and general multivariate relationships.\n",
    "\n",
    "Ranking features is really an effort to understand the features and their relationships with eachother.  We will start with basic data visualization and move to more complicated methods such are partial correlation and recursive feature elimination.\n",
    "\n",
    "### Ranking Method - Coverage\n",
    "\n",
    "Let's start with the concept of feature coverage. \n",
    "\n",
    "* If a feature is available over a small proportion of the samples then we may not want to include it as it will result in issues with feature imputation, estimation of missing data.\n",
    "\n",
    "* By removing a couple features with poor coverage we may improve our model because there are limitations with feature imputation, feature imputation can actually impose bias in statistics and additional error in our prediction models\n",
    "\n",
    "* if likewise deletion is applied to deal with missing values, features with low coverage result in a lot of removed data!\n",
    "\n",
    "Let's start with a bar chart with the proportion of missing records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(311)\n",
    "(df.isnull().sum()/len(df)).plot(kind = 'bar')          # calculate DataFrame with percentage missing by feature\n",
    "plt.xlabel('Feature'); plt.ylabel('Percentage of Missing Values'); plt.title('Data Completeness'); plt.ylim([0.0,1.0])\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=2.2, wspace=0.2, hspace=0.2) # plot formatting\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the provided example dataset the plot should be empty. There are no missing data so the 'Proportion of Missing Records' is 0.0 for all features. \n",
    "\n",
    "If you wanted to test this plot with some missing data, run this code first:\n",
    "\n",
    "```python\n",
    "proportion_NaN = 0.1                                    # proportion of values in DataFrame to remove\n",
    "\n",
    "remove = np.random.random(df.shape) < proportion_NaN    # make the boolean array for removal\n",
    "print('Fraction of removed values in mask ndarray = ' + str(round(remove.sum()/remove.size,3)) + '.')\n",
    "\n",
    "df_mask = df.mask(remove)                               # make a new DataFrame with specified proportion removed\n",
    "```\n",
    "\n",
    "Remove the code and reload the data to continue.\n",
    "\n",
    "This does not tell the whole story. For example, if 20% of feature A is missing and 20% of feature B is missing are those the same and different samples. This has a huge impact if you perform likewise deletion.\n",
    "\n",
    "If there is not too much data then we can actually visualize data coverage over all samples and features in a boolean table like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_missing_data = False\n",
    "if add_missing_data == True:                           # randomly remove data\n",
    "    prop_missing = 0.2\n",
    "    df = df.mask(np.random.random(df.shape)<prop_missing)\n",
    "\n",
    "\n",
    "df_temp = df.copy(deep=True)                            # make a deep copy of the DataFrame\n",
    "df_bool = df_temp.isnull()                              # true is value, false if NaN\n",
    "#df_bool = df_bool.set_index(df_temp.pop('UWI'))        # set the index / feature for the heat map y column\n",
    "heat = sns.heatmap(df_bool, cmap=['r','w'], annot=False, fmt='.0f',cbar=False,linecolor='black',linewidth=0.1) # make the binary heat map, no bins\n",
    "heat.set_xticklabels(heat.get_xticklabels(), rotation=90, fontsize=8)\n",
    "heat.set_yticklabels(heat.get_yticklabels(), rotation=0, fontsize=8)\n",
    " \n",
    "heat.set_title('Data Completeness Heatmap',fontsize=16); heat.set_xlabel('Feature',fontsize=12); heat.set_ylabel('Sample (Index)',fontsize=12)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.2, top=2.2, wspace=0.2, hspace=0.2) # plot formatting\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again this plot should be quite boring for the provided dataset with perfect coverage, every cell should be filled in red. \n",
    "\n",
    "* add the code to remove some records to test this plot. White cells are missing records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Imputation\n",
    "\n",
    "I don't cover feature imputation here.\n",
    "\n",
    "* check out my Python well-documented demonstration of methods for [Feature Imputation](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/SubsurfaceDataAnalytics_Feature_Imputation.ipynb)\n",
    "\n",
    "We will just apply likewise delection and move on.\n",
    "\n",
    "* we remove all samples with any missing feature values. While this is quite simple, it is a sledge hammer approach to ensure perfect coverage required by feature ranking methods that we are about to demonstrate. Please check out the other methods in the linked workflow above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(axis=0,how='any',inplace=True)                # likewise deletion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Method - Summary Statistics\n",
    "\n",
    "In any multivariate work we should start with the univariate analysis, summary statistics of one variable at a time. The summary statistic ranking method is qualitative, we are asking:\n",
    "\n",
    "* are there data issues?\n",
    "* do we trust the features? do we we trust the features all equally?\n",
    "* are there issues that need to be taken care of before we develop any multivariate workflows?\n",
    "\n",
    "There are a lot of efficient methods to calculate summary statistics from tabular data in DataFrames. The describe command provides count, mean, minimum, maximum, and quartiles all in a compact data table. We use transpose() command to flip the table so that features are on the rows and the statistics are on the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().transpose()                               # DataFrame summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary statistics are a critical first step in data checking. \n",
    "\n",
    "* this includes the number of valid (non-null) values for each feature (count removes all np.NaN from the totals for each variable).\n",
    "\n",
    "* we can see the general behavoirs such as central tendency, mean, and dispersion, variance. \n",
    "\n",
    "* we can identify issue with negative values, extreme values, and values that are outside the range of plausible values for each property. \n",
    "\n",
    "We have some negative TOC values! Let's check the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(111)\n",
    "GSLIB.hist_st(df['TOC'].values,-.2,2.2,log=False,cumul = False,bins=50,weights = None,xlabel='TOC (fraction)',title='Total Organic Carbon')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=0.7, top=0.8, wspace=0.1, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    There are just a couple slighly negative values, let's just truncate them at zero. We   can use this command below to set all TOC values in the DataFrame that are less than 0.0 as 0.0, otherwise we keep the original TOC value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TOC'] = np.where(df['TOC']<0.0, 0.0, df['TOC'])     # set TOC < 0.0 as 0.0, otherwise leave the same\n",
    "df['TOC'].describe().transpose()                        # summary statistics just for TOC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We can also establish the feature ranges for plotting.  We could calculate the feature range directly from the data with code like this:\n",
    "\n",
    "```p\n",
    "Pormin = np.min(df['Por'].values)          # extract ndarray of data table column\n",
    "Pormax = np.max(df['Por'].values)          # and calculate min and max\n",
    "```\n",
    "\n",
    "but, this would not result in easy to understand color bars and axis scales, let's pick convenient round numbers. We will also declare feature labels for ease of plotting. \n",
    "\n",
    "#### Set Up Lists to Assist with Our Workflow\n",
    "\n",
    "We build lists with names, units, minimum and maximum values for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == 0:\n",
    "    response = 'Prod'\n",
    "    \n",
    "    x = df.copy(deep = True); x = x.drop(['Well',response],axis='columns')\n",
    "    Y = df.loc[:,response]\n",
    "    \n",
    "    features = x.columns.values.tolist() + [Y.name]\n",
    "    pred = x.columns.values.tolist()\n",
    "    resp = Y.name\n",
    "    \n",
    "    xmin = [6.0,0.0,1.0,10.0,0.0,0.9]; xmax = [24.0,10.0,5.0,85.0,2.2,2.9]\n",
    "    Ymin = 500.0; Ymax = 9000.0\n",
    "    \n",
    "    predlabel = ['Porosity (%)','Permeability (mD)','Acoustic Impedance (kg/m2s*10^6)','Brittleness Ratio (%)',\n",
    "                 'Total Organic Carbon (%)','Vitrinite Reflectance (%)']\n",
    "    resplabel = 'Normalized Initial Production (MCFPD)'\n",
    "    \n",
    "    predtitle = ['Porosity','Permeability','Acoustic Impedance','Brittleness Ratio',\n",
    "                 'Total Organic Carbon','Vitrinite Reflectance']\n",
    "    resptitle = 'Normalized Initial Production'\n",
    "    \n",
    "    featurelabel = predlabel + [resplabel]\n",
    "    featuretitle = predtitle + [resptitle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks to be in pretty good shape and for brevity we skip outlier detection. Let's look at the distributions.\n",
    "\n",
    "### Ranking Method - Univariate Distributions\n",
    "\n",
    "As with summary statistics, this ranking method is a qualitative check for issues with the data and to assess our confidence with each feature. It is better to not include a feature with low confidence of quality as it may be misleading (while adding to model complexity as discussed previously).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbins = 20                                            # number of histogram bins\n",
    "\n",
    "for i, feature in enumerate(features):                # plot histograms with central tendancy and P10 and P90 labelled\n",
    "    plt.subplot(3,3,i+1)\n",
    "    y,_,_ = plt.hist(x=df[feature],weights=None,bins=nbins,alpha = 0.8,edgecolor='black',color='darkorange',density=True)\n",
    "    histogram_bounds(values=df[feature].values,weights=np.ones(len(df)),color='red')\n",
    "    plt.xlabel(feature); plt.ylabel('Frequency'); plt.ylim([0.0,y.max()*1.10])\n",
    "    plt.title(featuretitle[i])                     \n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3., top=3.1, wspace=0.2, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The univariate distributions look good:\n",
    "\n",
    "* there are no obvious outliers\n",
    "* the permeability is postively skewed as often observed\n",
    "* the corrected TOC has a small spike, but it's reasonable\n",
    "\n",
    "### Ranking Method - Bivariate Distributions\n",
    "\n",
    "Matrix scatter plots are a very efficient method to observe the bivarate relationships between the variables.  \n",
    "\n",
    "* this is another opportunity through data visualization to identify data issues\n",
    "* we can assess if we have collinearity, specifically simpler form between two features at a time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['c' + resp] = pd.cut(x=df[resp], bins=[0, 2000, 4000, 6000], # make a truncated response cateogorical feature\n",
    "                     labels=['Low', 'Mid', 'High'])\n",
    "\n",
    "nsample = len(df)                                       # set to less than number of samples for a speed up\n",
    "\n",
    "dpalette = sns.color_palette(\"rocket_r\",n_colors = 3)   # matrix scatter plot with points and density estimator\n",
    "palette = sns.color_palette(\"rocket\")\n",
    "matrixplot = sns.pairplot(df.sample(n=nsample),vars=features,hue ='c'+resp,diag_kind = 'kde',palette = dpalette,diag_kws={'edgecolor':'black'},plot_kws=dict(s=50, edgecolor=\"black\", linewidth=0.5,alpha=0.2))\n",
    "matrixplot.map_lower(sns.kdeplot, levels=3, color=\"black\")\n",
    "#plt.subplots_adjust(left=0.0, bottom=0.0, right=0.5, top=0.6, wspace=0.2, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This plot communicates a lot of information. How could we use this plot for variable ranking?\n",
    "\n",
    "* we can identify variables that are closely related to eachother.  e.g. if two variables have almost a perfect monotonic linear or near linear relationship we should remove one immediately.  This is a simple case of colinearity that will likley result in model instability as discussed above.\n",
    "\n",
    "* we can check for linear vs. non-linear relationships.  If we observe nonlinear bivariate relationships this will impact the choice of methods, and the quality of results from methods that assume linear relationships for variable ranking.\n",
    "\n",
    "* we can identify constraint relationships and heteroscedasticity between variables. Once again these may restrict our ranking methods and also encourage us to retains specific features to retain these features in the resulting model. \n",
    "\n",
    "Yet, we must remember that bivariate visualization and analysis is not sufficient to understand all the multivariate relationships in the data. Multicollinearity includes strong linear relationships between 2 or more features. These may be hard to see with only bivariate plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ranking Method - Pairwise Covariance\n",
    "\n",
    "Pairwise covariance provides a measure of the strength of the linear relationship between each predictor feature and the response feature. At this point, we specify that the goal of this study is to predict production, our response variable, from the other available predictor features.  We are thinking predictively now, not inferentially, we want to estimate the function, $\\hat{f}$, to accomplish this:\n",
    "\n",
    "\\begin{equation}\n",
    "Y = \\hat{f}(X_1,\\ldots,X_n) \n",
    "\\end{equation}\n",
    "\n",
    "where $Y$ is our response feature and $X_1,\\ldots,X_n$ are our predictor features.  If we retained all of our predictor features to predict the response we would have: \n",
    "\n",
    "\\begin{equation}\n",
    "Prod = \\hat{f}(Por,Perm,AI,Brittle,TOC,VR) \n",
    "\\end{equation}\n",
    "\n",
    "Now back to the covariance, the covariance is defined as:  \n",
    "\n",
    "\\begin{equation}\n",
    "C_{xy}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{(n-1)}\n",
    "\\end{equation}\n",
    "\n",
    "Covariance:\n",
    "* measures the linear relationship\n",
    "* sensitive to the dispersion / variance of both the predictor and response\n",
    "\n",
    "We can use the follow command to build a covariance matrix:\n",
    "\n",
    "```p\n",
    "df.iloc[:,1:8].cov()                                    # covariance matrix sliced predictors vs. response\n",
    "```\n",
    "the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with the covariances between all predictors features and the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covariance = df.iloc[:,1:8].cov().iloc[6,:6]            # calculate covariance matrix and slice for only pred - resp\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The covariance is useful, but as you can see the magnitude is quite variable. Most importantly these magnitudes are a function of each feature's variance. Feature variance is somewhat arbitrary. For example, what is the variance of porosity in fraction vs. percentage or permeaiblity in Darcy vs. milliDarcy. We can show that if we apply a constant multiplier, $c$, to a variable, $X$, that the variance will change according to this relationship (the proof is based on expectation formulation of variance):\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma_{cX}^2 = c^2 \\cdot \\sigma_{X}^2\n",
    "\\end{equation}\n",
    "\n",
    "By moving from percentage to fraction we decrease the variance of porosity by a factor of 10,000! The variance of each variable is potentially arbitrary, with the exception when all the features are in the same units. \n",
    "\n",
    "Pairwise correlations avoids this issue. \n",
    "\n",
    "### Ranking Method - Pairwise Correlation Coefficient\n",
    "\n",
    "Pairwise correlation coefficient provides a measure of the strength of the linear relationship between each predictor feature and the response feature.  \n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{xy}  = \\frac{\\sum_{i=1}^{n} (x_i - \\overline{x})(y_i - \\overline{y})}{(n-1)\\sigma_x \\sigma_y}, \\, -1.0 \\le \\rho_{xy} \\le 1.0\n",
    "\\end{equation}\n",
    "\n",
    "The correlation coefficient:\n",
    "\n",
    "* measures the linear relationship\n",
    "* removes the sensitiviety to the dispersion / variance of both the predictor and response features, by normalizing by the product of the standard deviation of each feature \n",
    "\n",
    "We can use the follow command to build a correlation matrix:\n",
    "\n",
    "```p\n",
    "df.iloc[:,1:8].corr()\n",
    "```\n",
    "the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with the correlations between all predictors features and the response.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = df.iloc[:,1:8].corr().iloc[6,:6]          # calculate correlation matrix and slice for only pred - resp\n",
    "print(correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is interesting. \n",
    "\n",
    "* We see that porosity, permeability and total organic carbon have the strongest linear relationships with production. \n",
    "* Acoustic impedance has weak negative relationships with production.\n",
    "* Brittleness is very close to 0.0. If you review the brittleness vs. production scatterplot, you'll observe a complicated non-linear relationship. There is a brittleness ratio sweetspot for production (rock that is not too soft nor too hard)!\n",
    "\n",
    "We could also look at the full correlation matrix to evaluate the potential for redudancy between predictor features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(111)                                        # plot correlation matrix with significance colormap\n",
    "sns.heatmap(df.iloc[:,1:7].corr(),vmin = -1.0, vmax = 1.0,linewidths=.5, fmt= '.1f',cmap = signif)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "* strong degree of correlation between porosity and permeability and porosity and TOC\n",
    "\n",
    "* strong degree of negative correlation between TOC and acoustic impedance\n",
    "\n",
    "We are still limited to a strick linear relationship.  The rank correlation allows us to relax this assumption.\n",
    "\n",
    "### Ranking Method - Pairwise Spearman Rank Correlation Coefficient\n",
    "\n",
    "The rank correlation coefficient applies the rank transform to the data prior to calculating the correlation coefficent. To calculate the rank transform simply replace the data values with the rank $R_x = 1,\\dots,n$, where $n$ is the maximum value and $1$ is the minimum value. \n",
    "\n",
    "\\begin{equation}\n",
    "\\rho_{R_x R_y}  = \\frac{\\sum_{i=1}^{n} (R_{x_i} - \\overline{R_x})(R_{y_i} - \\overline{R_y})}{(n-1)\\sigma_{R_x} \\sigma_{R_y}}, \\, -1.0 \\le \\rho_{xy} \\le 1.0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "x_\\alpha, \\, \\forall \\alpha = 1,\\dots, n, \\, | \\, x_i \\ge x_j \\, \\forall \\, i \\gt j \n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "R_{x_i} = i\n",
    "\\end{equation}\n",
    "\n",
    "The rank correlation:\n",
    "\n",
    "* measures the monotonic relationship, relaxes the linear assumption\n",
    "* removes the sensitiviety to the dispersion / variance of both the predictor and response, by normalizing by the product of the standard deviation of each. \n",
    "\n",
    "We can use the follow command to build a rank correlation matrix and calculate the p-value:\n",
    "\n",
    "```p\n",
    "stats.spearmanr(df.iloc[:,1:8])\n",
    "```\n",
    "the output is a new Pandas DataFrame, so we can slice the last column to get a Pandas series (ndarray with names) with \n",
    "the correlations between all predictors features and the response.\n",
    "\n",
    "Also, we get a very convenient *pval* 2D ndarry with the two-sided (two-tail summing semmetric over both tails) p-value for a hypothesis test with: \n",
    "\n",
    "\\begin{equation}\n",
    "H_o: \\rho_{R_x R_y} = 0\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "H_1: \\rho_{R_x R_y} \\ne 0\n",
    "\\end{equation}\n",
    "\n",
    "Let's keep the p-values between all the predictor features and our response feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank_correlation, rank_correlation_pval = stats.spearmanr(df.iloc[:,1:8]) # calculate the range correlation coefficient\n",
    "rank_correlation = rank_correlation[:,6][:6]\n",
    "rank_correlation_pval = rank_correlation_pval[:,6][:6]\n",
    "print(rank_correlation)\n",
    "print(rank_correlation_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some interesting results:\n",
    "\n",
    "* we almost doubled the correlation between Brittleness and Production be relaxing the linear assumption, of course it is still low due to the monotonic assumption in with the rank correlation.\n",
    "* we increased the correlation between porosity and production. See the slightly curve in the porosity and production constraint.\n",
    "* only the brittleness vs. production rank correlation coefficient is not significantly different than zero\n",
    "\n",
    "All of these methods up to now have considered one variable at a time. We can also consider methods that consider all features jointly to 'isolate' the influence of each feature.\n",
    "\n",
    "### Ranking Method - Partial Correlation Coefficient\n",
    "\n",
    "This is a linear correlation coefficient that controls for the effects all the remaining variables, $\\rho_{XY.Z}$ and $\\rho_{YX.Z}$ is the partial correlation between $X$ and $Y$, $Y$ and $X$, after controlling for $Z$.\n",
    "\n",
    "To calculate the partial correlation coefficient between $X$ and $Y$ given $Z_i, \\forall \\quad i = 1,\\ldots, m-1$ remaining features we use the following steps:\n",
    "\n",
    "1. perform linear, least-squares regression to predict $X$ from $Z_i, \\forall \\quad i = 1,\\ldots, m-1$. $X$ is regressed on the predictors to calculate the estimate, $X^*$\n",
    "\n",
    "2. calculate the residuals in Step \\#1, $X-X^*$, where $X^* = f(Z_{1,\\ldots,m-1})$, linear regression model\n",
    "\n",
    "3. perform linear, least-squares regression to predict $Y$ from $Z_i, \\forall \\quad i = 1,\\ldots, m-1$. $Y$ is regressed on the predictors to calculate the estimate, $Y^*$\n",
    "\n",
    "4. calculate the residuals in Step #3, $Y-Y^*$, where $Y^* = f(Z_{1,\\ldots,m-1})$, linear regression model\n",
    "\n",
    "5. calculate the correlation coefficient between the residuals from Steps #2 and #4, $\\rho_{X-X^*,Y-Y^*}$\n",
    "\n",
    "The partial correlation, provides a measure of the linear relationship between $X$ and $Y$ while controlling for the effect of $Z$ other features on both, $X$ and $Y$.  We use the function declared previously taken from Fabian Pedregosa-Izquierdo, f@bianp.net. The original code is on GitHub at https://git.io/fhyHB.\n",
    "\n",
    "To use this method we must assume:\n",
    "\n",
    "1. two variables to compare, $X$ and $Y$\n",
    "2. other variables to control, $Z_{1,\\ldots,m-2}$ \n",
    "3. linear relationships between all variables\n",
    "4. no significant outliers\n",
    "5. approximately bivariate normality between the variables\n",
    "\n",
    "We are in pretty good shape, but we have some departures from bivariate normality.  We could consider Gaussian univariate transforms to improve this.  This option is provided later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partial_correlation = partial_corr(df.iloc[:,1:8]) # calculate the partial correlation coefficients\n",
    "partial_correlation = partial_correlation[:,6][:6] # extract a single row and remove production with itself\n",
    "print(partial_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see a lot of new things about the unique contributions of each predictor feature! \n",
    "\n",
    "* porosity and permeability are strongly correlated with each other so they are penalized severely \n",
    "\n",
    "* acoustic impedance's and vitrinite reflectance's absolute correlation are increased reflecting their unique contributions\n",
    "\n",
    "* total organic carbon flipped signs!  When we control for all other variables, it has a negative relationship with production. \n",
    "\n",
    "With the partial correlation coefficients we have controlled for the influence of all other predictor features on both the specific predictor and the response features. The semipartial correlation filters out the influence of all other predictor features on the raw response variable. \n",
    "\n",
    "### Ranking Method - Semipartial Correlation Coefficient\n",
    "\n",
    "This is a linear correlation coefficient that controls for the effects all the remaining features, $Z$ on $X$, and then calculates the correlation between the residual $X^*-X$ and $Y$.  Note: we do not control for influence of $Z$ features on the response feature, $Y$. \n",
    "\n",
    "To calculate the semipartial correlation coefficient between $X$ and $Y$ given $Z_i, \\forall \\quad i = 1,\\ldots, m-1$ remaining features we use the following steps:\n",
    "\n",
    "1. perform linear, least-squares regression to predict $X$ from $Z_i, \\forall \\quad i = 1,\\ldots, m-1$. $X$ is regressed on the remaining predictor features to calculate the estimate, $X^*$\n",
    "\n",
    "2. calculate the residuals in Step \\#1, $X-X^*$, where $X^* = f(Z_{1,\\ldots,m-1})$, linear regression model\n",
    "\n",
    "3. calculate the correlation coefficient between the residuals from Steps #2 and $Y$ response feature, $\\rho_{X-X^*,Y}$\n",
    "\n",
    "The semipartial correlation coefficient, provides a measure of the linear relationship between $X$ and $Y$ while controlling for the effect of $Z$ other predictor features on the predictor feature, $X$, to get the unique contribution of $X$ with respect to $Y$. We use a modified version of the partial correlation function that we declared previously. The original code is on GitHub at https://git.io/fhyHB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semipartial_correlation = semipartial_corr(df.iloc[:,1:8]) # calculate the semi-partial correlation coefficients\n",
    "semipartial_correlation = semipartial_correlation[:,6][:6] # extract a single row and remove production with itself\n",
    "print(semipartial_correlation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More information to consider:\n",
    "\n",
    "* porosity, permeability and vitrinite reflectance are the most important by this feature ranking method\n",
    "* all other predictor features have quite low correlations\n",
    "\n",
    "This is a good moment to stop and take stock of all the results from the quantitative methods.  We will plot them all together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(151)\n",
    "plt.plot(pred,covariance,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Covariance')\n",
    "t = plt.title('Covariance')\n",
    "plt.ylim(-5000,5000)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.plot(pred,correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "t = plt.title('Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.plot(pred,rank_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Rank Correlation Coefficient')\n",
    "t = plt.title('Rank Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(154)\n",
    "plt.plot(pred,partial_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Partial Correlation Coefficient')\n",
    "t = plt.title('Partial Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(155)\n",
    "plt.plot(pred,semipartial_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Semipartial Correlation Coefficient')\n",
    "t = plt.title('Semipartial Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize All Bivariate Metrics \n",
    "\n",
    "Let's use heat maps to summarize all the feature pairwise metrics. We calculate the differences to aid in interpretation.\n",
    "\n",
    "* **partial correlation - correlation** - impact of confounding features on correlation\n",
    "* **mutual information - correlation** - impact of nonlinearity and heteroscedasticity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nsample = -1\n",
    "if nsample > 0:\n",
    "    df_sample = df.sample(n=nsample)\n",
    "else: \n",
    "    df_sample = df\n",
    "    \n",
    "plt.subplot(241)                          # plot a color coded correlation matrix\n",
    "df_corr = df_sample[features].corr()\n",
    "sns.heatmap(df_corr,vmin = -1.0, vmax = 1.0,linewidths=.5, fmt= '.1f',cmap = signif)\n",
    "plt.title('Pearson Correlation Coefficient')\n",
    "\n",
    "plt.subplot(242)                          # plot a color coded correlation matrix\n",
    "df_rank_corr = stats.spearmanr(df[features])[0]\n",
    "sns.heatmap(df_rank_corr,vmin = -1.0, vmax = 1.0,linewidths=.5, fmt= '.1f',cmap = signif)\n",
    "plt.title('Spearman Rank Correlation Coefficient')\n",
    "\n",
    "plt.subplot(243)                          # plot a color coded correlation matrix\n",
    "partial_correlation_temp = partial_corr(df[features]) # calculate the partial correlation coefficients\n",
    "df_partial = pd.DataFrame(partial_correlation_temp,columns=features,index=features)\n",
    "sns.heatmap(df_partial,vmin = -1.0, vmax = 1.0,linewidths=.5, fmt= '.1f',cmap = signif)\n",
    "plt.title('Partial Correlation Coefficient')\n",
    "\n",
    "plt.subplot(244)\n",
    "df_mutual = pd.DataFrame(mutual_matrix(df[features],features),columns=features,index=features)\n",
    "sns.heatmap(df_mutual,vmin = -1.0, vmax = 1.0,linewidths=.5, fmt= '.1f',cmap = signif)  \n",
    "plt.title('Mutual Information')\n",
    "\n",
    "plt.subplot(246)\n",
    "sns.heatmap(df_rank_corr-df_corr,vmin = -2.0, vmax = 2.0,linewidths=.5, fmt= '.1f',cmap = plt.cm.RdBu_r)  \n",
    "plt.title('Correlation Difference: Rank - Pearson')\n",
    "\n",
    "plt.subplot(247)\n",
    "sns.heatmap(df_partial-df_corr,vmin = -2.0, vmax = 2.0,linewidths=.5, fmt= '.1f',cmap = plt.cm.RdBu_r)  \n",
    "plt.title('Correlation Difference: Partial - Pearson')\n",
    "\n",
    "plt.subplot(248)\n",
    "plot = sns.heatmap(df_mutual-df_corr,vmin = -2.0, vmax = 2.0,linewidths=.5, fmt= '.1f',cmap = plt.cm.RdBu_r) \n",
    "plt.title('Correlation Difference: Mutual - Pearson')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=4.0, top=3.1, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think we are converging on porosity, permeability and vitrinite reflectance as the most important variables with respect to linear relationships with the production.  \n",
    "\n",
    "### Repeat with Standardization or Normal Score Transform of All Features\n",
    "\n",
    "Just as a check, let's standardize all the features and repeat the previously calculated quantitative methods. We know this will have an impact on covariance, what about the partial and semipartial correlations?\n",
    "\n",
    "There is a bunch of code to get this done, but it isn't too complicated. First, lets make a new DataFrame with all variables standardized. Then we can make a minor edit (change the DataFrame name) and reuse the code from above. You can choose between:\n",
    "\n",
    "1. Afine Correction - scale the distributions to have $\\overline{x} = 0$ and $\\sigma_x = 1.0$.\n",
    "2. Normal Score Transform - distribution transform of each feature to standard normal, Gaussian shape with $\\overline{x} = 0$ and $\\sigma_x = 1.0$.\n",
    "\n",
    "Use this block to perform affine correction of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfS = pd.DataFrame()                      # affine correction of each feature, standardization to a mean of 0 and variance of 1 \n",
    "# dfS['Well'] = df['Well'].values\n",
    "# dfS['Por'] = GSLIB.affine(df['Por'].values,0.0,1.0)\n",
    "# dfS['Perm'] = GSLIB.affine(df['Perm'].values,0.0,1.0)\n",
    "# dfS['AI'] = GSLIB.affine(df['AI'].values,0.0,1.0)\n",
    "# dfS['Brittle'] = GSLIB.affine(df['Brittle'].values,0.0,1.0)\n",
    "# dfS['TOC'] = GSLIB.affine(df['TOC'].values,0.0,1.0)\n",
    "# dfS['VR'] = GSLIB.affine(df['VR'].values,0.0,1.0)\n",
    "# dfS['Prod'] = GSLIB.affine(df['Prod'].values,0.0,1.0)\n",
    "# dfS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this block to perform normal score transform of the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfS = pd.DataFrame()                                    # Gaussian transform of each feature, standardization to a mean of 0 and variance of 1 \n",
    "dfS['Well'] = df['Well'].values\n",
    "dfS['Por'],d1,d2 = geostats.nscore(df,'Por')\n",
    "dfS['Perm'],d1,d2 = geostats.nscore(df,'Perm')\n",
    "dfS['AI'],d1,d2 = geostats.nscore(df,'AI')\n",
    "dfS['Brittle'],d1,d2 = geostats.nscore(df,'Brittle')\n",
    "dfS['TOC'],d1,d2 = geostats.nscore(df,'TOC')\n",
    "dfS['VR'],d1,d2 = geostats.nscore(df,'VR')\n",
    "dfS['Prod'],d1,d2 = geostats.nscore(df,'Prod')\n",
    "dfS.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regardless of transform that you chose it it good to check the summary statistics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfS.describe()                                          # check the summary statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also check the matrix scatter plot again.\n",
    "\n",
    "* If you performed normal score transform, you have standardized the mean and variance and correct the univariate shape of the distribution, but the bivariate relationships still depart from Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfS['c' + resp] = pd.cut(x=df[resp], bins=[0, 2000, 4000, 6000], # make a truncated response cateogorical feature\n",
    "                     labels=['Low', 'Mid', 'High'])\n",
    "\n",
    "nsample = len(df)                                       # set to less than number of samples for a speed up\n",
    "\n",
    "dpalette = sns.color_palette(\"rocket_r\",n_colors = 3)   # matrix scatter plot with points and density estimator\n",
    "palette = sns.color_palette(\"rocket\")\n",
    "matrixplot = sns.pairplot(dfS.sample(n=nsample),vars=features,hue ='c'+resp,diag_kind = 'kde',palette = dpalette,diag_kws={'edgecolor':'black'},plot_kws=dict(s=50, edgecolor=\"black\", linewidth=0.5,alpha=0.2))\n",
    "matrixplot.map_lower(sns.kdeplot, levels=3, color=\"black\")\n",
    "#plt.subplots_adjust(left=0.0, bottom=0.0, right=0.5, top=0.6, wspace=0.2, hspace=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the new DataFrame with standardized variables. Now we repeat the previous calculations. We will be more efficient this time and use quite compact code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stand_covariance = dfS.iloc[:,1:8].cov().iloc[6,:6]\n",
    "stand_correlation = dfS.iloc[:,1:8].corr().iloc[6,:6]\n",
    "stand_rank_correlation, stand_rank_correlation_pval = stats.spearmanr(dfS.iloc[:,1:8])\n",
    "stand_rank_correlation = stand_rank_correlation[:,6][:6]\n",
    "stand_rank_correlation_pval = stand_rank_correlation_pval[:,6][:6]\n",
    "stand_partial_correlation = partial_corr(dfS.iloc[:,1:8])\n",
    "stand_partial_correlation = stand_partial_correlation[:,6][:6]\n",
    "stand_semipartial_correlation = semipartial_corr(dfS.iloc[:,1:8])\n",
    "stand_semipartial_correlation = stand_semipartial_correlation[:,6][:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and repeat the previous summary plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.values[1:][:6]\n",
    "plt.subplot(151)\n",
    "plt.plot(features,stand_covariance,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Covariance')\n",
    "t = plt.title('Covariance')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.plot(features,stand_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "t = plt.title('Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.plot(features,stand_rank_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Rank Correlation Coefficient')\n",
    "t = plt.title('Rank Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(154)\n",
    "plt.plot(features,stand_partial_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Partial Correlation Coefficient')\n",
    "t = plt.title('Partial Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(155)\n",
    "plt.plot(features,stand_semipartial_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Semipartial Correlation Coefficient')\n",
    "t = plt.title('Semipartial Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What should you observe:\n",
    "\n",
    "* covariance is now equal to correlation coefficient\n",
    "* the semipartial correlations are sensitive to the feature standardization (affine correlation or normal score transform).\n",
    "\n",
    "### Conditional Statistics\n",
    "\n",
    "We will separate the wells into low, mid and high production and access the difference in the conditional statistics.  \n",
    "\n",
    "* This will provide a more flexible method to compare the relationship between each feature and production\n",
    "\n",
    "* If the conditional statistics change significantly then that feature is informative\n",
    "\n",
    "We are going to make a single violin plot over all of our features\n",
    "\n",
    "* We need a cateogrical feature for production, we truncate production to High or Low\n",
    "\n",
    "```python\n",
    "df['tProd'] = np.where(df['Prod']>=4000, 'High', 'Low') \n",
    "```\n",
    "\n",
    "* We will need to standardize all of our features so we can observe their relative differences together\n",
    "\n",
    "```python\n",
    "x = df[['Por','Perm','AI','Brittle','TOC','VR']]\n",
    "x_stand = (x - x.mean()) / (x.std())      \n",
    "```\n",
    "* This code extracted the features into a new DataFrame 'x', then applied the standardization operation on each column (feature)\n",
    "\n",
    "* Then we add the truncated production feature into the standardized features\n",
    "\n",
    "```python \n",
    "x = pd.concat([df['tProd'],x_stand.iloc[:,0:6]],axis=1)\n",
    "```\n",
    "\n",
    "* We can then apply the melt command to unpivot the DataFrame\n",
    "\n",
    "```python\n",
    "x = pd.melt(x,id_vars=\"tProd\",var_name=\"Predictors\",value_name='Standardized_Value')\n",
    "```\n",
    "\n",
    "* We now have a long DataFrame (6 features x 200 samples = 12000 rows) with:\n",
    "\n",
    "    * production: Low or High\n",
    "    * features: Por, Perm, AI, Brittle, TOC or VR\n",
    "    * standardized feature value\n",
    "    \n",
    "We can then build our violin plot \n",
    "\n",
    "* x is our predictor features\n",
    "* y is the standardized values for the predictor features (all now in one column)\n",
    "* hue is the production level High or Low\n",
    "* split is True so the violins are split in half\n",
    "* inner is quartiles for P25, P50 and P75 are plotted as dashed lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tProd'] = np.where(df['Prod']>=4000, 'High', 'Low') # make a high and low production categorical feature\n",
    "\n",
    "x_temp = df[pred]\n",
    "x_temp_stand = (x_temp - x_temp.mean()) / (x_temp.std())                    # standardization by feature\n",
    "x_temp = pd.concat([df['tProd'],x_temp_stand.iloc[:,0:6]],axis=1) # add the production categorical feature to the DataFrame\n",
    "x_temp = pd.melt(x_temp,id_vars=\"tProd\",var_name=\"Predictor Feature\",value_name='Standardized Predictor Feature') # unpivot the DataFrame\n",
    "\n",
    "plt.subplot(111)\n",
    "sns.violinplot(x=\"Predictor Feature\", y=\"Standardized Predictor Feature\", hue=\"tProd\", data=x_temp,split=True, inner=\"quart\", palette=\"Set2\")\n",
    "plt.xticks(rotation=90); plt.title('Conditional Distributions by Production')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the violin plot we can observe that the conditional distributions of porosity, permeability, TOC have the most variation between low and high production wells.\n",
    "\n",
    "We can replace the plot with box and whisker plots of the conditional distributions.  \n",
    "\n",
    "* Box and whisker plots improve our ability to observe outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplot(111)\n",
    "sns.boxplot(x=\"Predictor Feature\", y=\"Standardized Predictor Feature\", hue=\"tProd\", data=x_temp)\n",
    "plt.xticks(rotation=90)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.5, wspace=0.2, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "df = df.drop(['tProd'], axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the conditional box plot we can observe that the conditional distributions of porosity, permeability, TOC have the most variation between low and high production wells.\n",
    "\n",
    "* We can observed the outliers in porosity, permeability (upper tail), total organic carbon (lower tail) and vitrinite reflectance. \n",
    "\n",
    "### Variance Inflation Factor (VIF)\n",
    "\n",
    "A measure of linear multicolinearity between a predictor feature ($X_i$) a nd all other predictor features ($X_j, \\forall j \\ne i$).\n",
    "\n",
    "First we calculate a linear regression for a predictor feature given all the other predictor features.\n",
    "\n",
    "\\begin{equation}\n",
    "X_i = \\sum_{j, j \\ne i}^m X_j + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "From this model we determin the coefficient of determination, $R^2$, known as variance explained.\n",
    "\n",
    "Then we calculate the Variance Inflation Factor as:\n",
    "\n",
    "\\begin{equation}\n",
    "VIF = \\frac{1}{1 - R^2}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vif_values = []\n",
    "for i in range(x.shape[1]):\n",
    "    vif_values.append(variance_inflation_factor(x.values, i))\n",
    "\n",
    "vif_values = np.asarray(vif_values)\n",
    "indices = np.argsort(vif_values)[::1]                  # find indicies for descending order\n",
    "\n",
    "plt.subplot(111)                                        # plot the feature importance \n",
    "plt.title(\"Variance Inflation Factor\")\n",
    "plt.bar(range(x.shape[1]), vif_values[indices],edgecolor = 'black',\n",
    "       color=\"darkorange\",alpha=0.6, align=\"center\")\n",
    "plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90); \n",
    "\n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n",
    "\n",
    "plt.xlim([-0.5, x.shape[1]-0.5]); plt.yscale('log'); plt.ylim([1.0e1,1.0e4])\n",
    "plt.xlabel('Predictor Feature'); plt.ylabel('Variance Inflation Factor')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model-based Feature Ranking \n",
    "\n",
    "With these methods we evaluate the performance of the features in building an actual predictive model. Let's start with $B$ coefficients.\n",
    "\n",
    "### Ranking Method - $B$ Coefficients / Beta Weights \n",
    "\n",
    "We could also consider $B$ coefficients.  These are the linear regression coefficients without standardization of the variables. Let's use the linear regression method that is available in the SciPy package.\n",
    "\n",
    "The estimator for $Y$ is simply the linear equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Y^* = \\sum_{i=1}^{m} b_i X_i + c\n",
    "\\end{equation}\n",
    "\n",
    "The $b_i$ coefficients are solved to minimize the squared error between the estimates, $Y^*$ and the values in the training dataset, $Y$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(dfS[pred],dfS[resp])\n",
    "b = reg.coef_\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ouput is the $b$ coefficients, ordered over our features from $b_i, i = 1,\\ldots,n$ and then the intercept, $c$, that I have removed to avoid confusion.\n",
    "\n",
    "* we see the negative contribution of AI and TOC\n",
    "* the results are very sensitive to the magnitudes of the variances of the predictor features. \n",
    "\n",
    "We can remove this sensitivity by working with standardized features.\n",
    "\n",
    "### Ranking Method - $\\beta$ Coefficients / Beta Weights \n",
    "\n",
    "$\\beta$ coefficients are calculated as the linear regression of the coefficients after we have standardized the predictor and response features to have a variance of one.  \n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_{X^s_i} = 1.0 \\quad \\forall \\quad i = 1,\\ldots,m, \\quad \\sigma^2_{Y^s} = 1.0\n",
    "\\end{equation}\n",
    "\n",
    "The estimator for $Y^s$ standardized is simply the linear equation:\n",
    "\n",
    "\\begin{equation}\n",
    "Y^{s*} = \\sum_{i=1}^{m} \\beta_i X^s_i + c\n",
    "\\end{equation}\n",
    "\n",
    "It is convenient that we have just standardized all our variables to have a variance of 1.0 just recently (see above). Let's use the same linear regression method again on the standardized features to get $\\beta$ coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = LinearRegression()\n",
    "linear.fit(dfS[features],Y)\n",
    "beta = reg.coef_\n",
    "y_hat = linear.predict(dfS[features])\n",
    "\n",
    "indices = np.argsort(np.abs(beta))[::-1]             # find indicies for descending order\n",
    "\n",
    "plt.subplot(121)                                        # plot the feature importance \n",
    "plt.title(\"Linear Regression-based Feature importances\")\n",
    "plt.bar(range(x.shape[1]), beta[indices],edgecolor = 'black',\n",
    "       color=\"darkorange\",alpha = 0.6, align=\"center\")\n",
    "plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)\n",
    "plt.xlim([-0.5, x.shape[1]-0.5]); \n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n",
    "#plt.ylim([0.,1.0])\n",
    "plt.plot([-0.5,5.5],[0,0],color='black')\n",
    "plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\n",
    "\n",
    "plt.subplot(122)                                         # model cross validation plot\n",
    "plt.scatter(Y,y_hat,color='darkorange',edgecolor='black',alpha=0.7)\n",
    "plt.grid(visible=True,which='both',axis='both')\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(5)); plt.gca().yaxis.set_minor_locator(AutoMinorLocator(5)) \n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().xaxis.grid(True, which='major',linewidth = 1.0); plt.gca().xaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.xlim([Ymin,Ymax]); plt.ylim([Ymin,Ymax]); plt.xlabel('Truth: ' + response); plt.ylabel('Predicted:' + response)\n",
    "plt.plot([Ymin,Ymax],[Ymin,Ymax],color='black',ls='--'); plt.title('Predictive Model Cross Validation Plot')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "* the change between $b$ and $\\beta$ coefficients is not just a constant scaling\n",
    "\n",
    "* porosity, acoustic impedance and total organic carbon have the great weight in the estimate of production\n",
    "\n",
    "### Ranking Method - Feature Importance\n",
    "\n",
    "A variety of machine learning methods provide measures of feature importance.  \n",
    "\n",
    "* for exaple decision trees track the reduction in mean square error through inclusion of each feature\n",
    "\n",
    "Let's look at the feature importance from a random forest regressor fit to our data.\n",
    "\n",
    "* We will instantiate a random forest with default hyperparameters. This results in unlimited complexity, over-trained trees in our forest. The averaging of these trees takes care of the overfit issue.\n",
    "\n",
    "* Then we will train our random forest and extract the importances (expectation over all the trees)\n",
    "\n",
    "* we can also extract the feature importances over all the trees in the forest and summarize with the standard deviation to access the robustness of our feature importance measure\n",
    "\n",
    "For more information check out my lecture on [random forest](https://www.youtube.com/watch?v=m5_wk310fho&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=39) predictive machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code modified from https://www.kaggle.com/kanncaa1/feature-selection-and-data-visualization\n",
    "seed = 73093                                                # set the random forest hyperparameters\n",
    "max_depth = 7\n",
    "min_samples_leaf = 5\n",
    "num_tree = 1000\n",
    "max_features = 4\n",
    "random_forest = RandomForestRegressor(min_samples_leaf = min_samples_leaf,random_state=seed,n_estimators=num_tree, max_features=max_features)\n",
    "random_forest.fit(x,Y) # fit the random forest\n",
    "y_hat = random_forest.predict(x)\n",
    "importance_rank = random_forest.feature_importances_ # extract the expected feature importances\n",
    "importance_rank_stand = importance_rank/np.max(importance_rank)                          # calculate relative mutual information\n",
    "\n",
    "std = np.std([tree.feature_importances_ for tree in random_forest.estimators_],axis=0) # calculate stdev over trees\n",
    "indices = np.argsort(importance_rank)[::-1]             # find indicies for descending order\n",
    "\n",
    "plt.subplot(121)                                        # plot the feature importance \n",
    "plt.title(\"Random Forest-based Feature importances\")\n",
    "plt.bar(range(x.shape[1]), importance_rank[indices],edgecolor = 'black',\n",
    "       color=\"darkorange\",alpha = 0.6, yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)\n",
    "plt.xlim([-0.5, x.shape[1]-0.5]); \n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n",
    "plt.ylim([0.,1.0])\n",
    "plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\n",
    "\n",
    "plt.subplot(122)                                         # model cross validation plot\n",
    "plt.scatter(Y,y_hat,color='darkorange',edgecolor='black',alpha=0.7)\n",
    "plt.grid(visible=True,which='both',axis='both')\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator(5)); plt.gca().yaxis.set_minor_locator(AutoMinorLocator(5)) \n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().xaxis.grid(True, which='major',linewidth = 1.0); plt.gca().xaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.xlim([Ymin,Ymax]); plt.ylim([Ymin,Ymax]); plt.xlabel('Truth: ' + response); plt.ylabel('Predicted:' + response)\n",
    "plt.plot([Ymin,Ymax],[Ymin,Ymax],color='black',ls='--'); plt.title('Predictive Model Cross Validation Plot')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is more we can do with model-based methods. We will actually test models to assess the incremental impact of each predictor feature! We will try this with recursive feature elimination.\n",
    "\n",
    "Let's plot the results from the $B$ and $\\beta$ coefficients and compare with the previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(151)\n",
    "plt.plot(features,rank_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Rank Correlation Coefficient')\n",
    "t = plt.title('Rank Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(152)\n",
    "plt.plot(pred,partial_correlation,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Partial Correlation Coefficient')\n",
    "t = plt.title('Partial Correlation Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(153)\n",
    "plt.plot(features,b[0:6],color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('B Coefficients')\n",
    "t = plt.title('B Coefficient')\n",
    "plt.ylim(-5000,5000)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(154)\n",
    "plt.plot(features,beta[0:6],color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Beta Coefficient')\n",
    "t = plt.title('Beta Coefficient')\n",
    "plt.ylim(-1,1)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(155)\n",
    "plt.plot(features,importance_rank_stand,color='black')\n",
    "plt.plot([0.0,0.0,0.0,0.0,0.0,0.0],'r--',linewidth = 1.0)\n",
    "plt.xlabel('Predictor Features')\n",
    "plt.ylabel('Feature Importance')\n",
    "t = plt.title('Random Forest Feature Importance')\n",
    "plt.ylim(-1,1.0)\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.2, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "Mutual information is a generalized approach that quantifies the mutual dependence between two features.  \n",
    "\n",
    "* quantifies the amount of information gained from observing one feature about the other\n",
    "\n",
    "* avoids any assumption about the form of the relationship (e.g. no assumption of linear relationship)\n",
    "\n",
    "* compares the joint probabilities to the product of the marginal probabilities\n",
    "\n",
    "For discrete or binned continuous features $X$ and $Y$, mutual information is calculated as:\n",
    "\n",
    "\\begin{equation}\n",
    "I(X;Y) = \\sum_{y \\in Y} \\sum_{x \\in X}P_{X,Y}(x,y) log \\left( \\frac{P_{X,Y}(x,y)}{P_X(x) \\cdot P_Y(y)} \\right)\n",
    "\\end{equation}\n",
    "\n",
    "recall for indepedence:\n",
    "\n",
    "\\begin{equation}\n",
    "P_{X,Y}(x,y) = P_X(x) \\cdot P_Y(y)\n",
    "\\end{equation}\n",
    "\n",
    "therefore if the two features are independent then the $log \\left( \\frac{P_{X,Y}(x,y)}{P_X(x) \\cdot P_Y(y)} \\right) = 0$\n",
    "\n",
    "The joint probability $P_{X,Y}(x,y)$ is a weighting term on the sum and enforces closure.\n",
    "\n",
    "* parts of the joint distribution with greater density have greater impact on the mutual information metric\n",
    "\n",
    "For continuous (and nonbinned) features we can applied the integral form.\n",
    "\n",
    "\\begin{equation}\n",
    "I(X;Y) = \\int_{Y} \\int_{X}P_{X,Y}(x,y) log \\left( \\frac{P_{X,Y}(x,y)}{P_X(x) \\cdot P_Y(y)} \\right) dx dy\n",
    "\\end{equation}\n",
    "\n",
    "We get a sorted list of the indices in decreasing order of importance with the command\n",
    "\n",
    "```python\n",
    "indices = np.argsort(importances)[::-1]\n",
    "```\n",
    "\n",
    "the slice reverses the order, for descending order of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df.iloc[:,[1,2,3,4,5,6]]                            # separate DataFrames for predictor and response features\n",
    "y = df.iloc[:,[7]]\n",
    "\n",
    "mi = mutual_info_regression(x,np.ravel(y))              # calculate mutual information\n",
    "mi /= np.max(mi)                                        # calculate relative mutual information\n",
    "\n",
    "indices = np.argsort(mi)[::-1]                          # find indicies for descending order\n",
    "\n",
    "print(\"Feature ranking:\")                               # write out the feature importances\n",
    "for f in range(x.shape[1]):\n",
    "    print(\"%d. feature %s = %f\" % (f + 1, x.columns[indices][f], mi[indices[f]]))\n",
    "\n",
    "plt.subplot(111)                                        # plot the relative mutual information \n",
    "plt.title(\"Mutual Information\")\n",
    "plt.bar(range(x.shape[1]), mi[indices],edgecolor = 'black',\n",
    "       color=\"darkorange\",alpha=0.6,align=\"center\")\n",
    "plt.xticks(range(x.shape[1]), x.columns[indices],rotation=90)\n",
    "plt.xlim([-0.5, x.shape[1]-0.5]); plt.ylim([0,1.3])\n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n",
    "plt.xlabel('Predictor Feature'); plt.ylabel('Mutual Information')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information Accounting For Relevance and Redundancy\n",
    "\n",
    "The standard Maximum Relevance - Minumum Redundancy (MRMR) objective function considers a subset of predictor features, i.e., to score predictor feature subsets as metric to identify the most informative subset of predictor features.\n",
    "\n",
    "* the approach calculates the average mutual information between the subset of predictor features and the response feature minus the average mutual information between the subset of predictor features.\n",
    "\n",
    "\\begin{equation}\n",
    "MID = \\frac{1}{|S|}{\\sum_{\\alpha \\in S} I(X_{\\alpha},Y) } - \\frac{1}{|S|^2} {\\sum_{\\alpha \\in S}^m \\sum_{\\beta \\in S}^m I(X_{\\alpha},X_{\\beta})}\n",
    "\\end{equation}\n",
    "\n",
    "as a measure of $relevance - redundancy$ or \n",
    "\n",
    "\\begin{equation}\n",
    "MIQ = \\frac{ \\frac{1}{|S|}{\\sum_{\\alpha \\in S}^m I(X_{\\alpha},Y) } }{ \\frac{1}{|S|^2} {\\sum_{\\alpha \\in S}^m \\sum_{\\beta \\in S}^m I(X_{\\alpha},X_{\\beta})} }\n",
    "\\end{equation}\n",
    "\n",
    "* as a measure of $\\frac{relevance}{redundancy}$.\n",
    "\n",
    "I propose that for one-at-a-time predictor feature ranking (predictor feature subset, $S = [X_i]$ and $|S| = 1$) we modify this to the following calculation:\n",
    "\n",
    "* **relevance** - the mutual information between the selected predictor feature, $X_i$, and the response feature, $Y$\n",
    "* **redudancy** - the average mutual information between the selected predictor feature, $X_i$, and the remaining predictor features, $X_{\\alpha}, \\alpha \\ne i$.\n",
    "* we use the quotient form of the calculation from Gulgezen, Cataltepe and Yu (2009).\n",
    "\n",
    "Our modified version of the Maximum Relevance - Minumum Redundancy (MRMR) objective function for one-at-a-time feature ranking scores the selected predictor feature $X_i$'s **relavance** as its mutual information with the response feature:\n",
    "\n",
    "\\begin{equation}\n",
    "I(X_i,Y)\n",
    "\\end{equation}\n",
    "\n",
    "and **redundancy** between the selected predictor feature, $X_i$, and the remaining predictor features:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{|S|-1} \\sum_{\\alpha=1, \\alpha \\ne i}^m I(X_i,X_{\\alpha})\n",
    "\\end{equation}\n",
    "\n",
    "were $X$ are predictor features, $Y$ is the response feature, $X_i$ is the specific predictor feature being scored and $|S|$ is the number of predictor features and $I()$ is mutual information between the indicated features.\n",
    "\n",
    "\\begin{equation}\n",
    "\\Phi(X_i,Y) = \\frac{ I(X_i,Y) }{ \\frac{1}{|S|-1} \\sum_{\\alpha=1, \\alpha \\ne i}^m I(X_i,X_{\\alpha})}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_mutual = mutual_information_objective(x,Y)\n",
    "indices_obj = np.argsort(obj_mutual)[::-1] # find indicies for descending order\n",
    "\n",
    "plt.subplot(111)                          # plot the relative mutual information \n",
    "plt.title(\"One-at-a-Time MRMR Objective Function for Mutual Information-based Feature Selection\")\n",
    "plt.bar(range(x.shape[1]), obj_mutual[indices_obj],\n",
    "       color=\"darkorange\",alpha = 0.6, align=\"center\",edgecolor=\"black\")\n",
    "plt.xticks(range(x.shape[1]), x.columns[indices_obj],rotation=90)\n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n",
    "plt.xlim([-0.5, x.shape[1]-0.5]); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delta Mutual Information Quotient Accounting for Relevance and Redundancy\n",
    "\n",
    "We use the mutual information quotient from Gulgezen, Cataltepe and Yu (2009).\n",
    "\n",
    "The standard MRMR objective function that scores the subset of features' **relavance** between the subset of predictor features and the response feature:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{|S|}{\\sum_{\\alpha=1}^m I(X_{\\alpha},Y) } \n",
    "\\end{equation}\n",
    "\n",
    "and **redundancy** between the the subset of predictor features:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{1}{|S|^2} {\\sum_{\\alpha=1}^m \\sum_{\\beta=1}^m I(X_{\\alpha},X_{\\beta})}\n",
    "\\end{equation}\n",
    "\n",
    "To find the most informative subset of predictor features we must find the subset of features that maximize relevance while minimizing redundacy. We can accomplish this by mazimizing either of these two formulations.\n",
    "\n",
    "\\begin{equation}\n",
    "MID = \\frac{1}{|S|}{\\sum_{\\alpha=1}^m I(X_{\\alpha},Y) } - \\frac{1}{|S|^2} {\\sum_{\\alpha=1}^m \\sum_{\\beta=1}^m I(X_{\\alpha},X_{\\beta})}\n",
    "\\end{equation}\n",
    "\n",
    "or \n",
    "\n",
    "\\begin{equation}\n",
    "MIQ = \\frac{ \\frac{1}{|S|}{\\sum_{\\alpha=1}^m I(X_{\\alpha},Y) } }{ \\frac{1}{|S|^2} {\\sum_{\\alpha=1}^m \\sum_{\\beta=1}^m I(X_{\\alpha},X_{\\beta})} }\n",
    "\\end{equation}\n",
    "\n",
    "I suggest feature ranking through the calculation of the change in $MIQ$ via inclusion and removal of a specific predictor feature ($X_i$).\n",
    "\n",
    "\\begin{equation}\n",
    "\\Delta MIQ_i = \\frac{ \\frac{1}{|S|}{\\sum_{\\alpha=1}^m I(X_{\\alpha},Y) } }{ \\frac{1}{|S|^2} {\\sum_{\\alpha=1}^m \\sum_{\\beta=1}^m I(X_{\\alpha},X_{\\beta})} } - \\frac{ \\frac{1}{|S|}{\\sum_{\\alpha=1,\\alpha \\ne i}^m I(X_{\\alpha},Y) } }{ \\frac{1}{|S|^2} {\\sum_{\\alpha=1,\\alpha \\ne i}^m \\sum_{\\beta=1,\\beta \\ne i}^m I(X_{\\alpha},X_{\\beta})} }\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_mutual_information = delta_mutual_information_quotient(x,Y)\n",
    "\n",
    "#delta_mutual_information = (delta_mutual_information - np.min(delta_mutual_information))/(np.max(delta_mutual_information) - np.min(obj_mutual))\n",
    "indices_delta_mutual_information = np.argsort(delta_mutual_information)[::-1]            # find indicies for descending order\n",
    "\n",
    "plt.subplot(111)                          # plot the relative mutual information \n",
    "plt.title(\"Delta Mutual Information Quotient\")\n",
    "plt.bar(range(x.shape[1]), delta_mutual_information[indices_delta_mutual_information],\n",
    "       color=\"darkorange\",alpha = 0.6,align=\"center\",edgecolor = 'black')\n",
    "plt.xticks(range(x.shape[1]), x.columns[indices_delta_mutual_information],rotation=90)\n",
    "plt.xlim([-0.5, x.shape[1]-0.5])\n",
    "plt.gca().yaxis.grid(True, which='major',linewidth = 1.0); plt.gca().yaxis.grid(True, which='minor',linewidth = 0.2) # add y grids\n",
    "plt.gca().tick_params(which='major',length=7); plt.gca().tick_params(which='minor', length=4)\n",
    "plt.gca().xaxis.set_minor_locator(AutoMinorLocator()); plt.gca().yaxis.set_minor_locator(AutoMinorLocator()) # turn on minor ticks\n",
    "plt.plot([-0.5,x.shape[1]-0.5],[0,0],color='black',lw=3); plt.xlabel('Predictor Feature'); plt.ylabel('Feature Importance')\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2., top=1., wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare Delta Mutual Information and Variance Inflation Factor\n",
    "\n",
    "Both of these methods account for predictor feature redundancy\n",
    "\n",
    "* VIF assumes linearity and does not account for relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(stats.rankdata(delta_mutual_information),stats.rankdata(-vif_values),c='darkorange',edgecolor='black')\n",
    "for i, feature in enumerate(x.columns):\n",
    "    plt.annotate(feature, (stats.rankdata(delta_mutual_information)[i]-0.5,stats.rankdata(-vif_values)[i]+0.3))\n",
    "plt.xlabel('Delta Mutual Information Rank'); plt.ylabel('Variance Inflation Factor Rank'); plt.title('Variance Inflation Factor vs. Delta Mutual Information Ranking')\n",
    "plt.xlim(0,len(x.columns)+1); plt.ylim(0,len(x.columns)+1)\n",
    "plt.plot([2,8],[0,6],color='black',alpha=0.5,ls='--'); \n",
    "plt.plot([0,6],[2,8],color='black',alpha=0.5,ls='--')\n",
    "#plt.annotate('Nonlinearity Reduces Rank',(1,11),c='grey',rotation = 36,fontsize = 15)\n",
    "#plt.annotate('or Relevance Reduces Rank',(3,10),c='grey',rotation = 36,fontsize = 15)\n",
    "plt.fill_between([0,6], [2,8], [8,8], color='grey',alpha=0.2,zorder=1)\n",
    "#plt.annotate('Nonlinearity Increases Range',(10,3),c='grey',rotation = 36,fontsize = 15)\n",
    "#plt.annotate('or Relevance Increases Rank',(11,1),c='grey',rotation = 36,fontsize = 15)\n",
    "plt.fill_between([2,8], [0,6], [0,0], color='grey',alpha=0.2,zorder=1)\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=1.5, top=1.7, wspace=0.1, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From mutual information we can observe that porosity, permeability then totoal organic carbon adn brittleness have the greatest departure from general independence.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a wide array of criteria to rank our features.  \n",
    "\n",
    "* the $B$ coefficient have the same issue as covariance, sensitivity to the univariate variance\n",
    "\n",
    "* the $\\beta$ coefficients remove this sensitivity and are consistent with previous results.\n",
    "\n",
    "Given all of these methods, I would rank the variables as:\n",
    "\n",
    "1. Porosity\n",
    "2. Vitrinite Reflectance\n",
    "3. Acoustic Impedance\n",
    "4. Permeability\n",
    "5. Total Organic Carbon\n",
    "6. Brittleness\n",
    "\n",
    "I have assigned these ranks by observing the general trend in these matrics.  Of course, we could make a more quantitative score and rank by weighting each method.\n",
    "\n",
    "As mentioned before, we should not neglect expert knowledge. If additional information is known about physical processes, causation, and reliability and availability of variables this should be integrated into assigning ranks.\n",
    "\n",
    "We include a bonus method here, recursive feature elimination, but only provide a simple linear regression model example. More could be done with more complicated models.\n",
    "\n",
    "### Ranking Method - Recursive Feature Elimination \n",
    "\n",
    "Recursive Feature Elimination (RFE) method works by recursively removing features and building a model with the remaining features.\n",
    "\n",
    "* for the first step, all features are used to build a model and the features are ranked by feature importance or the coeficient\n",
    "\n",
    "* the least important feature is pruned and the model is rebuilt\n",
    "\n",
    "* this is repeated until there is only one feature remaining\n",
    "\n",
    "In this code we make a prediction model based on multilinear regression and indicate that we want to find the best feature based on recursive feature elimination. The algorithm assigns rank $1,\\ldots,m$ for all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe = RFE(LinearRegression(),n_features_to_select=1,verbose=0) # set up RFE linear regression model\n",
    "df['const'] = np.ones(len(df))            # let's add one's for the constant term\n",
    "rfe = rfe.fit(df[pred].values,np.ravel(df[resp])) # recursive elimination\n",
    "dfS = df.drop('const',axis = 1)           # remove the ones\n",
    "print(rfe.ranking_[0:6])                  # print the variable ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive feature elimination method with a linear regression model provides these ranks:\n",
    "\n",
    "1. Total Organic Carbon\n",
    "2. Vitrinite Reflectance\n",
    "3. Acoustic Impedance\n",
    "4. Porosity\n",
    "5. Permeability\n",
    "6. Brittleness\n",
    "\n",
    "The advantages with the recursive elimination method:\n",
    "\n",
    "* the actual model can be used in assessing feature ranks\n",
    "* the ranking is based on accuracy of the estimate\n",
    "\n",
    "but this method is sensitive to:\n",
    "\n",
    "* choice of model\n",
    "* training dataset\n",
    "\n",
    "The feature ranks are quite different from our previous methods.  Many have moved from the previous assessment. Perhaps we should use a more flexible modeling method.\n",
    "\n",
    "Let's repeat this method with a more flexible machine learning method, decision tree regressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')            \n",
    "\n",
    "rfe = RFE(RandomForestRegressor(max_depth=3),n_features_to_select=1,verbose=0)      # set up RFE linear regression model\n",
    "df['const'] = np.ones(len(df))                        # let's add one's for the constant term\n",
    "\n",
    "lab_enc = preprocessing.LabelEncoder(); Y_encoded = lab_enc.fit_transform(Y)\n",
    "\n",
    "rfe = rfe.fit(x,np.ravel(Y_encoded))                  # recursive elimination\n",
    "dfS = df.drop('const',axis = 1)                       # remove the ones\n",
    "print(rfe.ranking_[0:6])                              # print the variable ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The recursive feature elimination method with a linear regression model provides these ranks:\n",
    "\n",
    "1. Porosity\n",
    "2. Brittleness\n",
    "3. Vitrinite Reflectance\n",
    "4. Permeability\n",
    "5. Total Organic Carbon\n",
    "6. Acoustic Impedance\n",
    "\n",
    "This method may be applied with cross validation (k fold iteration of training and testing datasets).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Comments\n",
    "\n",
    "This was a basic demonstration of multivariate feature ranking. A lot more could be done:  \n",
    "\n",
    "* recursive feature elimination with more complicated models and cross validation\n",
    "* principal components analysis for feature selection (mean of component loadings over all principal components for each feature)\n",
    "* consideration of the spatial context of the data\n",
    "* statistical testing, assignment of statistical significance and use of p-values\n",
    "\n",
    "I have other demonstrations on the basics of working with DataFrames, ndarrays, univariate statistics, plotting data, declustering, data transformations, trend modeling and many other workflows available at https://github.com/GeostatsGuy/PythonNumericalDemos and https://github.com/GeostatsGuy/GeostatsPy. \n",
    "  \n",
    "I hope this was helpful,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "#### The Author:\n",
    "\n",
    "### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "*Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions*\n",
    "\n",
    "With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers' and geoscientists' impact in subsurface resource development. \n",
    "\n",
    "For more about Michael check out these links:\n",
    "\n",
    "#### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "#### Want to Work Together?\n",
    "\n",
    "I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n",
    "\n",
    "* Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I'd be happy to drop by and work with you! \n",
    "\n",
    "* Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\n",
    "\n",
    "* I can be reached at mpyrcz@austin.utexas.edu.\n",
    "\n",
    "I'm always happy to discuss,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin\n",
    "\n",
    "#### More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
