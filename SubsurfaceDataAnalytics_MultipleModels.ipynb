{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "    <img src=\"https://github.com/GeostatsGuy/GeostatsPy/blob/master/TCG_color_logo.png?raw=true\" width=\"220\" height=\"240\" />\n",
    "\n",
    "</p>\n",
    "\n",
    "## Machine Learning Introduction  \n",
    "\n",
    "### Making a Variety of Machine Learning Models in Python, Model Training and Tuning \n",
    "\n",
    "#### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "\n",
    "##### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1) | [GeostatsPy](https://github.com/GeostatsGuy/GeostatsPy)\n",
    "\n",
    "### Multiple Machine Learning Models for Subsurface Modeling in Python \n",
    "\n",
    "Here's a simple workflow, demonstration of training multiple machine learning models with a single dataset.  \n",
    "\n",
    "* This is provided as a fast comparison of general behavoir of these models\n",
    "\n",
    "* There is no effort to tune the hyperparameters / test the models with withheld testing data\n",
    "\n",
    "* Given the 2 predictor features and 1 response feature it is easy to visualize the data and the model\n",
    "\n",
    "This provides an opportunity to run and visualize a variety of machine learning models for experiential learning.\n",
    "\n",
    "#### Machine Learning-based Prediction\n",
    "\n",
    "Predictive machine learning methods with supervised learning: \n",
    "\n",
    "**Prediction**\n",
    "\n",
    "* non-parametric method for regression and classification\n",
    "\n",
    "* a function $\\hat{f}$ of the nearest $k$ training data in predictor feature space such that we predict a response feature $Y$ from a set of predictor features $X_1,\\ldots,X_m$. \n",
    "\n",
    "* the prediction is of the form $\\hat{Y} = \\hat{f}(X_1,\\ldots,X_m)$ \n",
    "\n",
    "**Suppervised Learning**\n",
    "\n",
    "* the response feature label, $Y$, is available over the training and testing data\n",
    "\n",
    "#### The Machine Learning Models\n",
    "\n",
    "This workflow includes the following models:\n",
    "\n",
    "* linear regression\n",
    "* k-nearest neighbor regression\n",
    "* decision tree regression\n",
    "* random forest regression\n",
    "* tree-based gradient boosting regression  \n",
    "\n",
    "#### Getting Started\n",
    "\n",
    "Here's the steps to get setup in Python with the GeostatsPy package:\n",
    "\n",
    "1. Install Anaconda 3 on your machine (https://www.anaconda.com/download/). \n",
    "2. From Anaconda Navigator (within Anaconda3 group), go to the environment tab, click on base (root) green arrow and open a terminal. \n",
    "3. In the terminal type: pip install geostatspy. \n",
    "4. Open Jupyter and in the top block get started by copy and pasting the code block below from this Jupyter Notebook to start using the geostatspy functionality. \n",
    "\n",
    "If you want to work locally you will need to first copy the data file to your working directory. It is available here:\n",
    "\n",
    "* Tabular data - unconv_MV.csv found [here](https://github.com/GeostatsGuy/GeoDataSets/blob/master/unconv_MV.csv).\n",
    "\n",
    "You don't have to download the data if you want to load directly from my github account with the code below.\n",
    "\n",
    "#### Import Required Packages\n",
    "\n",
    "We will also need some standard packages. These should have been installed with Anaconda 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os                                                   # to set current working directory \n",
    "import math                                                 # basic calculations like square root\n",
    "import pandas as pd                                         # DataFrames and plotting\n",
    "import numpy as np                                          # arrays and matrix math\n",
    "import matplotlib.pyplot as plt                             # plotting\n",
    "import matplotlib.ticker as ticker                          # modify the plot axis labels\n",
    "from sklearn.model_selection import train_test_split        # train and test split\n",
    "from sklearn.pipeline import Pipeline                       # machine learning modeling pipeline\n",
    "from sklearn.model_selection import GridSearchCV            # model hyperparameter grid search\n",
    "from sklearn.model_selection import KFold                   # k-fold cross validation\n",
    "from sklearn.preprocessing import StandardScaler            # feature standardization\n",
    "np.random.seed(seed=73038)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a package import error, you may have to first install some of these packages. This can usually be accomplished by opening up a command window on Windows and then typing 'python -m pip install [package-name]'. More assistance is available with the respective package docs.  \n",
    "\n",
    "#### Declare functions\n",
    "\n",
    "Let's define a couple of convenience functions for model visualization and hyperparameter tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model,xtrain,xtest,x_min,x_max,ytrain,ytest,y_min,y_max,rtrain,rtest,z_min,z_max,title,colorbar=True):# plots the data points and the decision tree prediction \n",
    "    n_classes = 10\n",
    "    #cmap = plt.cm.RdYlBu\n",
    "    cmap = plt.cm.plasma\n",
    "    plot_step = max((x_max-x_min),(y_max-y_min))/200\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = np.clip(Z,z_min,z_max)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    #cs = plt.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n",
    "    cs = plt.imshow(Z,interpolation = None,extent = [x_min,x_max,y_min,y_max], vmin = z_min, vmax = z_max,cmap = cmap)\n",
    "\n",
    "    im = plt.scatter(xtrain,ytrain,s=None, c=rtrain, marker='o', cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.5, verts=None, edgecolors=\"white\",label='train')\n",
    "    im2 = plt.scatter(xtest,ytest,s=None, c=rtest, marker='s', cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.5, verts=None, edgecolors=\"black\",label='test')\n",
    "    plt.xlim([x_min,x_max]); plt.ylim([y_min,y_max])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xtrain.name)\n",
    "    plt.ylabel(ytrain.name)\n",
    "    plt.legend(loc = 'upper right')\n",
    "    if colorbar == True:\n",
    "      cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "      cbar.set_label(rtrain.name, rotation=270, labelpad=20)\n",
    "        \n",
    "def visualize_model_sub(ax,model,xtrain,xtest,x_min,x_max,ytrain,ytest,y_min,y_max,rtrain,rtest,z_min,z_max,title,legend=True):# plots the data points and the decision tree prediction \n",
    "    n_classes = 10\n",
    "    cmap = plt.cm.RdYlBu\n",
    "    cmap = plt.cm.plasma\n",
    "    plot_step = 0.02\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
    "                     np.arange(y_min, y_max, plot_step))\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = np.clip(Z,z_min,z_max)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    cs = ax.contourf(xx, yy, Z, cmap=cmap,vmin=z_min, vmax=z_max, levels=np.linspace(z_min, z_max, 100))\n",
    "    im = ax.scatter(xtrain,ytrain,s=None, c=rtrain, marker='o', cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.5, verts=None, edgecolors=\"white\",label='train')\n",
    "    im2 = ax.scatter(xtest,ytest,s=None, c=rtest, marker='s', cmap=cmap, norm=None, vmin=z_min, vmax=z_max, alpha=0.8, linewidths=0.5, verts=None, edgecolors=\"black\",label='test')   \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(xtrain.name)\n",
    "    ax.set_ylabel(ytrain.name)\n",
    "    ax.set_xlim(x_min,x_max)\n",
    "    ax.set_ylim(y_min,y_max)\n",
    "    if legend == True:\n",
    "      cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "      cbar.set_label(rtrain.name, rotation=270, labelpad=20)    \n",
    "        \n",
    "def tune_model(model,X,y,par_name,par_min,par_max,npar,spar,log_scale,int_list,title,xlabel):\n",
    "    scorelist = []\n",
    "    if log_scale:\n",
    "        parlist = np.logspace(math.log10(par_min),math.log10(par_max),npar)\n",
    "    else: \n",
    "        if int_list == True:\n",
    "            step = int((par_max-par_min)/npar)\n",
    "            parlist = np.arange(par_min,par_max+1,step,dtype = int)\n",
    "        else:\n",
    "            parlist = np.linspace(par_min,par_max,npar)\n",
    "\n",
    "    pipe = Pipeline([                                           # the machine learning workflow as a pipeline object\n",
    "        ('model', model)\n",
    "    ])\n",
    "\n",
    "    params = {                                                  # the machine learning workflow method's parameters\n",
    "        'model__' + str(par_name): parlist,\n",
    "    }\n",
    "    folds = 4\n",
    "    grid_cv_tuned = GridSearchCV(pipe, params, scoring = 'neg_mean_squared_error', # grid search cross validation \n",
    "                             cv=KFold(n_splits=folds,shuffle=False),\n",
    "                             refit = True)\n",
    "\n",
    "    grid_cv_tuned.fit(X,y)                                      # fit model with tuned hyperparameters        \n",
    "        \n",
    "    scorelist = abs(grid_cv_tuned.cv_results_['mean_test_score'])   \n",
    "    \n",
    "    if log_scale:    \n",
    "        plt.arrow(parlist[np.argmin(scorelist)],0,0,np.max(scorelist)*1.2,width=parlist[np.argmin(scorelist)]/10.0,color='red',head_length=0.0,head_width=0.0)    \n",
    "        plt.arrow(spar,0,0,np.max(scorelist)*1.2,width=spar/10.0,color='black',head_length=0.0,head_width=0.0)\n",
    "    else:\n",
    "        plt.arrow(parlist[np.argmin(scorelist)],0,0,np.max(scorelist)*1.2,width=1,color='red',head_length=0.0,head_width=0.0)    \n",
    "        plt.arrow(spar,0,0,np.max(scorelist)*1.2,width=0.5,color='black',head_length=0.0,head_width=0.0)\n",
    "    \n",
    "    plt.annotate('Your Model',[spar,np.max(scorelist)*0.95],rotation=270.0)\n",
    "    if int_list == True:\n",
    "        plt.annotate('Tuned Model, ' + xlabel + ' = ' + str(format(parlist[np.argmin(scorelist)],\"d\")),[parlist[np.argmin(scorelist)],np.max(scorelist)*0.4],rotation=270.0)\n",
    "    else:\n",
    "        plt.annotate('Tuned Model, ' + xlabel + ' = ' + str(format(parlist[np.argmin(scorelist)],\"4.3f\")),[parlist[np.argmin(scorelist)],np.max(scorelist)*0.4],rotation=270.0)\n",
    "\n",
    "        \n",
    "    plt.scatter(parlist,scorelist,s=5, c=\"red\", marker=None, cmap=None, norm=None, vmin=None, vmax=None, alpha=0.8, linewidths=0.5, verts=None, edgecolors=\"black\")\n",
    "    y_labels = plt.gca().get_yticks()\n",
    "    plt.gca().yaxis.set_major_formatter(ticker.FormatStrFormatter('%0.0e'))\n",
    "    plt.title(title); plt.xlabel(xlabel); plt.ylabel('Mean Square Error')\n",
    "    if log_scale:\n",
    "        plt.xscale('log'); \n",
    "    plt.xlim(par_min,par_max); plt.ylim(1.0,np.max(scorelist)*1.2)\n",
    "    \n",
    "def check_model(model,xtrain,xtest,ytrain,ytest,rtrain,rtest,rmin,rmax,title):    # plots the estimated vs. the actual  \n",
    "    predict_train = model.predict(np.c_[xtrain,ytrain])\n",
    "    predict_test = model.predict(np.c_[xtest,ytest])\n",
    "    plt.scatter(rtrain,predict_train,s=None, c='red',marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=0.3, verts=None, edgecolors=\"black\", label = 'train')\n",
    "    plt.scatter(rtest,predict_test,s=None, c='blue',marker='s', cmap=None, norm=None, vmin=None, vmax=None, alpha=0.2, linewidths=0.3, verts=None, edgecolors=\"black\", label = 'test')\n",
    "    plt.title(title); plt.xlabel('Actual Production (MCFPD)'); plt.ylabel('Estimated Production (MCFPD)')\n",
    "    plt.xlim(rmin,rmax); plt.ylim(rmin,rmax)\n",
    "    plt.legend(loc = 'upper right')\n",
    "    plt.arrow(rmin,rmin,rmax,rmax,width=0.02,color='black',head_length=0.0,head_width=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the working directory\n",
    "\n",
    "I always like to do this so I don't lose files and to simplify subsequent read and writes (avoid including the full address each time).\n",
    "\n",
    "* I include this for convience if you want to write out results or load the data file locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.chdir(\"c:/PGE383\")                                       # set the working directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will have to update the part in quotes with your own working directory and the format is different on a Mac (e.g. \"~/PGE\").  \n",
    "\n",
    "#### Read the data table\n",
    "\n",
    "We use a data \"unconv_MV.csv\" comma delimited file from my GitHub [respository](https://github.com/GeostatsGuy/GeoDataSets).  \n",
    "\n",
    "* this code loads the data directly from my GitHub repository.\n",
    "* if you have the data file available locally (i.e. you are not connected to the internet) set the working directory to the location with the data file and use the second line below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = pd.read_csv(\"https://raw.githubusercontent.com/GeostatsGuy/GeoDataSets/master/unconv_MV.csv\")   # load the comma delimited data file\n",
    "#my_data = pd.read_csv(\"unconv_MV.csv\")                     # load the comma delimited data file locally\n",
    "my_data = my_data.sample(frac = 0.2)                        # reduce the number of data for testing and visualization\n",
    "num = my_data._get_numeric_data()                           # get the numerical values\n",
    "production_error = 1500                                     # adjust the error in the response feature\n",
    "my_data['Production'] = my_data['Production'] + np.random.normal(loc=0.0,scale=production_error,size=len(my_data))\n",
    "num[num < 0] = 0                                            # truncate negative values to 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the first several rows of our data stored in a DataFrame so we can make sure we successfully loaded the data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.head(n=3)                                           # preview the first 3 rows of the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remove the well index and check the summary summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = my_data.iloc[:,1:8]                               # copy all rows and columns 1 through 8, note 0 column is removed\n",
    "my_data.describe().transpose()                              # calculate summary statistics for the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardizing Predictor Features\n",
    "\n",
    "The k-nearest neighbour etc. methods uses a nearest training sample search in feature space (like k-means clustering). To remove the impact feature range from the approach we standardize the features.\n",
    "\n",
    "* we will standardize our predictor features to have a mean of zero and a variance of one.\n",
    "\n",
    "* we use the scikikt learn preprocessing to simplify this step and provid a reverse transform.\n",
    "\n",
    "We also extract the selected, transformed features as convenient DataFrames for the next workflow steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "transform = StandardScaler(); \n",
    "features = ['Por','Brittle']\n",
    "\n",
    "my_data['Standardized Porosity'] = transform.fit_transform(my_data.loc[:,features].values)[:,0] # standardize the data features to mean = 0, var = 1.0\n",
    "my_data['Standardized Brittleness'] = transform.fit_transform(my_data.loc[:,features].values)[:,1] # standardize the data features to mean = 0, var = 1.0\n",
    "\n",
    "X1 = my_data['Standardized Porosity']\n",
    "X2 = my_data['Standardized Brittleness']\n",
    "X = my_data[['Standardized Porosity','Standardized Brittleness']]\n",
    "y = my_data['Production']\n",
    "my_data.head(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the output above with the original porosity and brittleness. The reverse transform works! We will use it to return to the original feature units.\n",
    "\n",
    "#### Feature Ranges\n",
    "\n",
    "Let's set some ranges for plotting. Note for the standardized predictor features we will use -3.5 to 3.5 as the limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pormin = 5.0; pormax = 25.0                                 # set minumums and maximums for visualization \n",
    "brittlemin = 0.0; brittlemax = 100.0\n",
    "prodmin = 0.0; prodmax = 9000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has variables from 1,000 unconventional wells including well average porosity, log transform of permeability (to linearize the relationships with other variables), accoustic impedance (kg/m2s*10^6), brittness ratio (%), total organic carbon (%), vitrinite reflectance (%), and initial production 90 day average (MCFPD).  Note, the dataset is synthetic.\n",
    "\n",
    "#### Working with Only Two Predictor Features\n",
    "\n",
    "Let's simplify the problem to 2 predictor features, Porosity and Brittleness to predict Production rate.  We will also reduce the number of wells from 1,000 to 500.  By working with only 2 features, it is very easy to visualize the segmentation of the feature space (it is only 2D and can be shown compleltely on a single plot)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data_subset = my_data.iloc[:,[7,8,6]]                    # extract just por, brittle and prod with 100 samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.25, random_state=73073)\n",
    "X1_train = X_train.iloc[:,0]; X2_train = X_train.iloc[:,1]; X1_test = X_test.iloc[:,0]; X2_test = X_test.iloc[:,1]; \n",
    "y_train = pd.DataFrame({'Production':y_train.values})       # force the response feature to be a DataFrame instead of a 1D ndarray\n",
    "y_test = pd.DataFrame({'Production':y_test.values})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check the univariate statistics of Porosity, Brittleness and Producton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(231)\n",
    "plt.hist(X1_train, alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Porosity Training Data (%)'); plt.xlim(-3,3)\n",
    "\n",
    "plt.subplot(232)\n",
    "plt.hist(X_train.iloc[:,1], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Britteness Training Data (%)'); plt.xlim(-3,3)\n",
    "\n",
    "plt.subplot(233)\n",
    "plt.hist(y_train.iloc[:,0], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Production Training Data (MCFPD)'); plt.xlim(prodmin,prodmax)\n",
    "\n",
    "plt.subplot(234)\n",
    "plt.hist(X_test.iloc[:,0], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Porosity Testing Data (%)'); plt.xlim(-3,3)\n",
    "\n",
    "plt.subplot(235)\n",
    "plt.hist(X_test.iloc[:,1], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Britteness Testing Data (%)'); plt.xlim(-3,3)\n",
    "\n",
    "plt.subplot(236)\n",
    "plt.hist(y_test.iloc[:,0], alpha = 0.2, color = 'red', edgecolor = 'black', bins=20)\n",
    "plt.title('Production Testing Data (MCFPD)'); plt.xlim(prodmin,prodmax)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=2.2, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distributions are well behaved, we cannot observe obvious gaps nor truncations.  Let's look at a scatter plot of Porosity vs. Brittleness with points colored by Production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "im = plt.scatter(X_train.iloc[:,0],X_train.iloc[:,1],s=None, c=y_train.iloc[:,0], marker=None, cmap=plt.cm.plasma, norm=None, vmin=prodmin, vmax=prodmax, alpha=0.8, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "plt.title('Training Production vs. Brittleness and Porosity'); plt.xlabel('Porosity (%)'); plt.ylabel('Brittleness (%)')\n",
    "plt.xlim(-3,3); plt.ylim(-3,3)\n",
    "cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "cbar.set_label(\"Production\", rotation=270, labelpad=20)\n",
    "\n",
    "plt.subplot(122)\n",
    "im = plt.scatter(X_test.iloc[:,0],X_test.iloc[:,1],s=None, c=y_test.iloc[:,0], marker=None, cmap=plt.cm.plasma, norm=None, vmin=prodmin, vmax=prodmax, alpha=0.8, linewidths=0.3, verts=None, edgecolors=\"black\")\n",
    "plt.title('Testing Production vs. Brittleness and Porosity'); plt.xlabel('Porosity (%)'); plt.ylabel('Brittleness (%)')\n",
    "plt.xlim(-3,3); plt.ylim(-3,3)\n",
    "cbar = plt.colorbar(im, orientation = 'vertical')\n",
    "cbar.set_label(\"Production\", rotation=270, labelpad=20)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem looks complicated and could not be modeled with simple linear regression.  It appears there is a sweet spot for Brittleness and increasing Porosity is always beneficial for Production.\n",
    "\n",
    "#### Play with a Variety of Machine Learning Models\n",
    "\n",
    "Let's try out several machine learning prediction models to predict production with the porosity and brittleness data.\n",
    "\n",
    "Here's our machine learning prediction model:\n",
    "\n",
    "\\begin{equation}\n",
    "production = f(porosity,brittleness) + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "We will show use the following models:\n",
    "\n",
    "* multilinear regression [Lecture](https://www.youtube.com/watch?v=0fzbyhWiP84&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=21)\n",
    "* ridge regression [Lecture](https://www.youtube.com/watch?v=pMGO40yXZ5Y&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=22)\n",
    "* k-nearest nearbours [Lecture](https://www.youtube.com/watch?v=lzmeChSYvv8&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=30)\n",
    "* decision tree [Lecture](https://www.youtube.com/watch?v=JUGo1Pu3QT4&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=38)\n",
    "* random forest [Lecture](https://www.youtube.com/watch?v=m5_wk310fho&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=39)\n",
    "* gradient boosting [Lecture](https://www.youtube.com/watch?v=___T8_ixIwc&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=40)\n",
    "\n",
    "check out the links for lectures on my YouTube channel for each. \n",
    "\n",
    "* I have a lot of lectures available on machine learning in the Subsurface Machine Learning [Playlist](https://www.youtube.com/watch?v=5kBS5ThMHcU&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf) \n",
    "\n",
    "___\n",
    "\n",
    "#### Instantiate, Fit and Predict with Multilinear Regression\n",
    "\n",
    "Let's instantiate, fit and predict with a linear regression model.\n",
    "\n",
    "Trying Out Hyperparameters:\n",
    "\n",
    "* there are no hyperparameters to try out with linear regression, standard linear regression does not have an option to tune model complexity / flexibility\n",
    "* changing the norm from L2 (not commonly done, but demonstrated [here](https://github.com/GeostatsGuy/PythonNumericalDemos/blob/master/Interactive_Norms.ipynb)) changes sensitivity to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model = LinearRegression()                            # instantiate the prediction model\n",
    "linear_model.fit(X_train,y_train)                    # train the model with the training data\n",
    "\n",
    "plt.subplot(121)\n",
    "visualize_model(linear_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Multilinear Regression Model')\n",
    "     \n",
    "plt.subplot(122)    \n",
    "check_model(linear_model,X_train.iloc[:,0],X_test.iloc[:,0],X_train.iloc[:,1],X_test.iloc[:,1],y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Multilinear Regression Cross Validation Plot')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=2.0, top=1.2, wspace=0.3, hspace=0.2)\n",
    "plt.show()\n",
    "\n",
    "from sklearn import metrics                                 # measures to check our models\n",
    "predict_test = linear_model.predict(np.c_[X_test.iloc[:,0],X_test.iloc[:,1]])\n",
    "print('Model parameters are = ') \n",
    "print('  Slope X1 = ' + str(round(linear_model.coef_[0][0],2)) + ' and slope X2 = ' + str(round(linear_model.coef_[0][1],2)))\n",
    "print('  Intercept = ' + str(round(linear_model.intercept_[0],2)))\n",
    "print('Testing mean square error = ' + str(round(metrics.mean_squared_error(predict_test,y_test.iloc[:,0]),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate, Fit and Predict with Ridge Regression\n",
    "\n",
    "Let's instantiate, fit and predict with a linear regression model.\n",
    "\n",
    "Trying Out Hyperparameters:\n",
    "\n",
    "* $\\lambda$ - the shrinkage factor applied to the shrinkage penalty, penalizes large absolute slope terms \n",
    "\n",
    "* linear regression alternative with hyperparamter tuning through regularization and feature selection:\n",
    "   \n",
    "    * LASSO [Lecture](https://www.youtube.com/watch?v=cVFYhlCCI_8&list=PLG19vXLQHvSC2ZKFIkgVpI9fCjkN38kwf&index=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lam = 0.003                                                  # hyperparameter\n",
    "lam_min = 1.0e-10; lam_max = 1.0e10; nlam = 20                           \n",
    "\n",
    "ridge_model = Ridge(alpha = lam)                             # instantiate the prediction model\n",
    "ridge_model.fit(X_train,y_train)                             # train the model with the training data\n",
    "\n",
    "plt.subplot(131)\n",
    "visualize_model(ridge_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Ridge Regression, Lambda = ' + str(lam))\n",
    "    \n",
    "plt.subplot(132)    \n",
    "check_model(ridge_model,X_train.iloc[:,0],X_test.iloc[:,0],X_train.iloc[:,1],X_test.iloc[:,1],y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Ridge Regression Cross Validation Plot, Lambda = ' + str(lam))\n",
    "    \n",
    "plt.subplot(133)\n",
    "tune_model(ridge_model,X,y,'alpha',lam_min,lam_max,nlam,lam,True,False,'Ridge Regression Hyperparameter Tuning','Lambda')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate, Fit and Predict with $k$ Nearest Neighbour\n",
    "\n",
    "Let's instantiate, fit and predict with a $k$ nearest neighbour model.\n",
    "\n",
    "* instantiate it with the hyperparameters, $k$ nearest neighbours \n",
    "\n",
    "* train with the training data, we use the standard fit function from scikit learn \n",
    "\n",
    "Trying Out Hyperparameters:\n",
    "\n",
    "* **k** - number of nearest training data to use for the estimate\n",
    "* **low k** - high model complexity / flexibility\n",
    "* **high k** - low model complexity / flexibility\n",
    "\n",
    "Just change this line of code below:\n",
    "\n",
    "```python\n",
    "k = 5                                                       # hyperparameter\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor          \n",
    "\n",
    "k = 4                                                      # hyperparameter\n",
    "k_min = 1; k_max=150; nk = 80                         \n",
    "\n",
    "neigh_model = KNeighborsRegressor(weights = 'uniform', n_neighbors=k, p = 1) # instantiate the prediction model\n",
    "neigh_model.fit(X_train,y_train)                            # train the model with the training data\n",
    "\n",
    "plt.subplot(131)\n",
    "visualize_model(neigh_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'k-Nearest Neighbours, k = ' + str(k))\n",
    "    \n",
    "plt.subplot(132)    \n",
    "check_model(neigh_model,X_train.iloc[:,0],X_test.iloc[:,0],X_train.iloc[:,1],X_test.iloc[:,1],y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'k-Nearest Neighbours Cross Validation Plot, k = ' + str(k))\n",
    "\n",
    "plt.subplot(133)\n",
    "tune_model(neigh_model,X,y,'n_neighbors',k_min,k_max,nk,k,False,True,'k-nearest Neighbours Hyperparameter Tuning','k')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate, Fit and Predict with Decision Tree Regression\n",
    "\n",
    "Let's instantiate, fit and predict with a decision tree regression model.\n",
    "\n",
    "Trying Out Hyperparameters:\n",
    "\n",
    "* **max_leaf_nodes** - maximum number of regions, terminal nodes, in the decision tree, low **max_lead_nodes** - low model complexity / flexibility\n",
    "* **min_samples_leaf** - minimum number of training data in a regions, terminal nodes, in the decision tree, low **min_samples_leaf** - high model complexity / flexibility\n",
    "* **max_depth** - maximum number of decision tree layers, low **max_depth** - low modelcomplexity / flexibility\n",
    "\n",
    "Just change these lines of code below:\n",
    "\n",
    "```python\n",
    "max_leaf_nodes = 20                                       # hyperparameters\n",
    "min_samples_leaf = 2\n",
    "max_depth = 10\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree                                   # tree program from scikit learn (package for machine learning)\n",
    "\n",
    "nodes = 80                                                 # hyperparameters\n",
    "nodes_min = 2; nodes_max=100; nnodes = 80                         \n",
    "\n",
    "tree_model = tree.DecisionTreeRegressor(max_leaf_nodes = nodes) # instantiate the prediction model\n",
    "tree_model.fit(X_train,y_train)                            # train the model with the training data\n",
    "\n",
    "plt.subplot(131)\n",
    "visualize_model(tree_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Decision Tree, Leaf Nodes = ' + str(nodes))\n",
    "    \n",
    "plt.subplot(132)    \n",
    "check_model(tree_model,X_train.iloc[:,0],X_test.iloc[:,0],X_train.iloc[:,1],X_test.iloc[:,1],y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Decision Tree Cross Validation Plot, Leaf Nodes = ' + str(nodes))\n",
    "    \n",
    "plt.subplot(133)\n",
    "tune_model(tree_model,X,y,'max_leaf_nodes',nodes_min,nodes_max,nnodes,nodes,False,True,'Decision Tree Hyperparameter Tuning','Leaf Nodes')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate, Fit and Predict with Tree-based Random Forest Regression\n",
    "\n",
    "Let's instantiate, fit and predict with a random forest regression model.\n",
    "\n",
    "Trying Out Hyperparameters:\n",
    "\n",
    "* **max_leaf_nodes** - maximum number of regions, terminal nodes, in the individual decision trees\n",
    "\n",
    "Other Model Hyperparameters:\n",
    "\n",
    "* **max_depth** - maximum number of decision tree layers, low **max_depth** - low model complexity / flexibility\n",
    "* **n_estimators** - the number of decision trees to average to make the ensemble estimate\n",
    "* **max_features** - number of features to randomly select for each decision, if 2 you will get tree bagging regression\n",
    "\n",
    "Just change these lines of code below:\n",
    "\n",
    "```python\n",
    "fnodes = 80                                               # hyperparameters\n",
    "max_depth = 10                                            \n",
    "n_estimators = 1000\n",
    "max_features = 2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor        \n",
    "\n",
    "fnodes = 20                                              # hyperparameters\n",
    "fnodes_min = 2; fnodes_max=100; fnnodes = 40              \n",
    "\n",
    "max_depth = 10                                            \n",
    "n_estimators = 10\n",
    "max_features = 2\n",
    "\n",
    "forest_model = RandomForestRegressor(max_leaf_nodes=fnodes,random_state=73073,n_estimators=n_estimators, max_features=max_features)\n",
    "forest_model.fit(X_train,y_train.iloc[:,0])                         # train the model with training data \n",
    "\n",
    "plt.subplot(131)\n",
    "visualize_model(forest_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Random Forest, Leaf Nodes = ' + str(fnodes))\n",
    "         \n",
    "plt.subplot(132)    \n",
    "check_model(forest_model,X_train.iloc[:,0],X_test.iloc[:,0],X_train.iloc[:,1],X_test.iloc[:,1],y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Random Forest Cross Validation Plot, Leaf Nodes = ' + str(fnodes))\n",
    "  \n",
    "plt.subplot(133)\n",
    "tune_model(forest_model,X,y,'max_leaf_nodes',fnodes_min,fnodes_max,fnnodes,fnodes,False,True,'Random Forest Hyperparameter Tuning','Leaf Nodes')\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instantiate, Fit and Predict with Tree-based Gradient Boosting Regression\n",
    "\n",
    "Let's instantiate, fit and predict with a tree-based gradient boosting regression model.\n",
    "\n",
    "Trying Out Hyperparameters:\n",
    "\n",
    "* **loss** - the loss function, 'ls' for least squares, L2 norm solution\n",
    "* **max_depth** - maximum number of decision tree layers, low **max_depth** - low model complexity / flexibility for each additive model for less ability to capture feature interactions and slower learning \n",
    "* **learning_rate** - too fast and the solution may be unstable, diverge, and too slow and the solution make take too long\n",
    "\n",
    "Just change these lines of code below:\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    'loss': 'ls',                                           # L2 Norm - least squares\n",
    "    'max_depth': 2,                                         # maximum depth of each tree\n",
    "    'learning_rate': 0.003,                                 # learning learning\n",
    "    'criterion': 'mse'                                      # individual tree loss\n",
    "}\n",
    "```\n",
    "\n",
    "Note, the above is a dictionary, a convenient Python class to flexibly store information, like the model hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor      # tree-based gradient boosting\n",
    "\n",
    "learning_rate = 0.003                                      # hyperparamters\n",
    "learning_rate_min = 0.000001; learning_rate_max = 1.0; learning_rate_n = 20\n",
    "\n",
    "gnodes = 100                                                 \n",
    "# gnodes_min = 2; gnodes_max=150; gnnodes = 20\n",
    "\n",
    "params = {\n",
    "    'loss': 'ls',                                           # L2 Norm - least squares\n",
    "    'max_leaf_nodes': gnodes,                               # maximum depth of each tree\n",
    "    'learning_rate': learning_rate,                         # constraint on magnitude of update with each estimator / for slow, robust learning\n",
    "    'criterion': 'mse'                                      # tree construction criteria is mean square error over training\n",
    "}\n",
    "\n",
    "gb_model = GradientBoostingRegressor(n_estimators=1000,**params) # instantiate the prediction model\n",
    "gb_model.fit(X_train,y_train['Production'])                    # train the model with the training data\n",
    "\n",
    "plt.subplot(131)\n",
    "visualize_model(gb_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Gradient Boosting, Learning Rate = ' + str(learning_rate))\n",
    " \n",
    "plt.subplot(132)    \n",
    "check_model(gb_model,X_train.iloc[:,0],X_test.iloc[:,0],X_train.iloc[:,1],X_test.iloc[:,1],y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Gradient Boosting Cross Validation Plot, Learning Rate = ' + str(learning_rate))\n",
    " \n",
    "plt.subplot(133)\n",
    "tune_model(gb_model,X,y,'learning_rate',learning_rate_min,learning_rate_max,learning_rate_n,learning_rate,True,False,'Gradient Boosting Hyperparameter Tuning','Learning Rate')\n",
    "#tune_model(gb_model,X,y,'max_leaf_nodes',gnodes_min,gnodes_max,gnnodes,gnodes,False,True,'Decision Tree Hyperparameter Tuning','Leaf Nodes')\n",
    "     \n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=3.0, top=1.2, wspace=0.2, hspace=0.2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n",
    "Let's look at all our models together for a visual comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "fig = plt.figure(constrained_layout=False)\n",
    "gs = gridspec.GridSpec(1,6, figure=fig,width_ratios = [1,1,1,1,1,1.2] ) \n",
    "\n",
    "visualize_model_sub(plt.subplot(gs[0]),linear_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Linear Model',legend=False)\n",
    "\n",
    "visualize_model_sub(plt.subplot(gs[1]),ridge_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Ridge Model, Lambda = ' + str(lam),legend=False)\n",
    "\n",
    "visualize_model_sub(plt.subplot(gs[2]),neigh_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'k-nearest Neighbour, k = ' + str(k),legend=False)\n",
    "\n",
    "visualize_model_sub(plt.subplot(gs[3]),tree_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Decision Tree, Leaf Nodes = ' + str(nodes),legend=False)\n",
    "\n",
    "visualize_model_sub(plt.subplot(gs[4]),forest_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Random Forest, Leaf Nodes = ' + str(fnodes),legend=False)\n",
    "\n",
    "visualize_model_sub(plt.subplot(gs[5]),gb_model,X_train.iloc[:,0],X_test.iloc[:,0],-3.5,3.5,X_train.iloc[:,1],X_test.iloc[:,1],-3.5,3.5,y_train.iloc[:,0],y_test.iloc[:,0],prodmin,prodmax,'Gradient Boosting, Leaf Nodes = ' + str(gnodes),legend=True)\n",
    "\n",
    "plt.subplots_adjust(left=0.0, bottom=0.0, right=6, top=1.2, wspace=0.15, hspace=0.2)\n",
    "\n",
    "fig.savefig('Multiple_ML_Models.jpg',dpi = 400, bbox_inches = 'tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "\n",
    "We just ran fast to build a variety of machine learning models with model parameter training and hyperparameter tuning. \n",
    "\n",
    "* I hope this was a help demonstration.\n",
    "\n",
    "* See the linked lectures at the beginning of this workflow for much more information about machine learning.\n",
    "\n",
    "I hope this was helpful,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin\n",
    "On twitter I'm the @GeostatsGuy.\n",
    "\n",
    "\n",
    "***\n",
    "\n",
    "#### More on Michael Pyrcz and the Texas Center for Geostatistics:\n",
    "\n",
    "### Michael Pyrcz, Associate Professor, University of Texas at Austin \n",
    "*Novel Data Analytics, Geostatistics and Machine Learning Subsurface Solutions*\n",
    "\n",
    "With over 17 years of experience in subsurface consulting, research and development, Michael has returned to academia driven by his passion for teaching and enthusiasm for enhancing engineers' and geoscientists' impact in subsurface resource development. \n",
    "\n",
    "For more about Michael check out these links:\n",
    "\n",
    "#### [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n",
    "\n",
    "#### Want to Work Together?\n",
    "\n",
    "I hope this content is helpful to those that want to learn more about subsurface modeling, data analytics and machine learning. Students and working professionals are welcome to participate.\n",
    "\n",
    "* Want to invite me to visit your company for training, mentoring, project review, workflow design and / or consulting? I'd be happy to drop by and work with you! \n",
    "\n",
    "* Interested in partnering, supporting my graduate student research or my Subsurface Data Analytics and Machine Learning consortium (co-PIs including Profs. Foster, Torres-Verdin and van Oort)? My research combines data analytics, stochastic modeling and machine learning theory with practice to develop novel methods and workflows to add value. We are solving challenging subsurface problems!\n",
    "\n",
    "* I can be reached at mpyrcz@austin.utexas.edu.\n",
    "\n",
    "I'm always happy to discuss,\n",
    "\n",
    "*Michael*\n",
    "\n",
    "Michael Pyrcz, Ph.D., P.Eng. Associate Professor The Hildebrand Department of Petroleum and Geosystems Engineering, Bureau of Economic Geology, The Jackson School of Geosciences, The University of Texas at Austin\n",
    "\n",
    "#### More Resources Available at: [Twitter](https://twitter.com/geostatsguy) | [GitHub](https://github.com/GeostatsGuy) | [Website](http://michaelpyrcz.com) | [GoogleScholar](https://scholar.google.com/citations?user=QVZ20eQAAAAJ&hl=en&oi=ao) | [Book](https://www.amazon.com/Geostatistical-Reservoir-Modeling-Michael-Pyrcz/dp/0199731446) | [YouTube](https://www.youtube.com/channel/UCLqEr-xV-ceHdXXXrTId5ig)  | [LinkedIn](https://www.linkedin.com/in/michael-pyrcz-61a648a1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
